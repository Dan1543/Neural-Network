{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "3JDc_vCt4Lkj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1ATJTpO4zb_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FUNCIONES DE ACTIVACION Y SUS DERIVADAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g19AzOYTXYc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clases de cada funcion (contiene su funcion y su derivada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "AmUUjdrl4qSr"
   },
   "outputs": [],
   "source": [
    "class activationFunction:\n",
    "    \"\"\"\n",
    "    Class for activation functions meant to be used on Neural Networks\n",
    "    All the activation functions and its derivatives are available on the subclases\n",
    "    Subclasses:\n",
    "    Swish()\n",
    "    Relu()\n",
    "    Purelin()\n",
    "    Logsig()\n",
    "    Tansig()\n",
    "    Radbas()\n",
    "    Tribas()\n",
    "    RadBasN()\n",
    "    HardLim()\n",
    "    HardLims()\n",
    "    SatLin()\n",
    "    SatLins()\n",
    "    Softmax()\n",
    "    LeakyRelu()\n",
    "    ELU()\n",
    "    GELU()\n",
    "    PReLU()\n",
    "    SELU()\n",
    "    SiLU()\n",
    "    Softplus()\n",
    "    \"\"\"\n",
    "    def function(self,x):\n",
    "        \"\"\"Activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def derivative(self,x):\n",
    "        \"\"\"Derivative of the activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def active(self):\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "vl52Q1Rl4sbo"
   },
   "outputs": [],
   "source": [
    "class Swish(activationFunction):\n",
    "    \"\"\"Scaled Exponential Linear Unit With a Shift function\"\"\"\n",
    "    def __init__(self, beta=1):\n",
    "        self.beta = beta\n",
    "    def function(self, x):\n",
    "        return x * (1 / (1 + np.exp(-self.beta * x)))\n",
    "    def derivative(self, x):\n",
    "        return (self.beta * self.function(x)) + (1 / (1 + np.exp(-self.beta * x))) * (1 - self.beta * self.function(x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "8shE1T3W4yJz"
   },
   "outputs": [],
   "source": [
    "class Relu(activationFunction):\n",
    "    \"\"\"Rectified linear unit function (ReLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,0)\n",
    "    def active(self):\n",
    "        out = [0, float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "j2v-Y_2495Yb"
   },
   "outputs": [],
   "source": [
    "class Purelin(activationFunction):\n",
    "    \"\"\"Linear (Identity) function\"\"\"\n",
    "    def function(self,x):\n",
    "      return x\n",
    "    def derivative(self,x):\n",
    "      return np.ones_like(x)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "KuoUYoUuIG6P"
   },
   "outputs": [],
   "source": [
    "class Logsig(activationFunction):\n",
    "    \"\"\"Logistic function\"\"\"\n",
    "    def function(self, x):\n",
    "      return 1 / (1 + np.exp(-x))\n",
    "    def derivative(self,x):\n",
    "        return self.function(x) * (1 - self.function(x))\n",
    "    def active(self):\n",
    "        out = [-4.0, 4.0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "FbhNfFvmI9Cd"
   },
   "outputs": [],
   "source": [
    "class Tansig(activationFunction):\n",
    "    \"\"\"Hyperbolic function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.tanh(x)\n",
    "    def derivative(self,x):\n",
    "        return  1- np.tanh(x)**2\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "jIZd2WWFKioZ"
   },
   "outputs": [],
   "source": [
    "class Radbas(activationFunction):\n",
    "    \"\"\"Gaussian function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.exp(-x**2)\n",
    "    def derivative(self,x):\n",
    "        return -2 * x * np.exp(-x**2)\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "UTE_IxFnLIkU"
   },
   "outputs": [],
   "source": [
    "class Tribas(activationFunction):\n",
    "    \"\"\"Triangular basis function\"\"\"\n",
    "    def function(self, x):\n",
    "      return np.maximum(0, 1 - np.abs(x))\n",
    "    def derivative(self, x):\n",
    "      return np.where(np.abs(x) < 1, -1, 0)\n",
    "    def active(self):\n",
    "        out = [-1, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "lgC1nhgnLjeX"
   },
   "outputs": [],
   "source": [
    "class RadBasN(activationFunction):\n",
    "    \"\"\"Normalized radial basis function\"\"\"\n",
    "    def __init__(self, sigma=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        sigma : float by default 1\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def function(self, x):\n",
    "      return np.exp(-0.5 * (x / self.sigma)**2)\n",
    "    def derivative(self, x):\n",
    "      return -x / self.sigma**2 * np.exp(-0.5 * (x / self.sigma)**2)\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "yhHg-ZxGNLkp"
   },
   "outputs": [],
   "source": [
    "class HardLim(activationFunction):\n",
    "    \"\"\"Hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)\n",
    "    def active(self):\n",
    "        out = [0, 0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "Vx0uAWHENjcZ"
   },
   "outputs": [],
   "source": [
    "class HardLims(activationFunction):\n",
    "    \"\"\"Symmetric hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)\n",
    "    def active(self):\n",
    "        out = [0, 0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "s44mofomN2ju"
   },
   "outputs": [],
   "source": [
    "class SatLin(activationFunction):\n",
    "    \"\"\"Saturatin linear function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, 0, None)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-0, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "3W5XynwgN-wb"
   },
   "outputs": [],
   "source": [
    "class SatLins(activationFunction):\n",
    "    \"\"\"Symmetric saturating function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, -1, 1)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(np.logical_and(x >= -1, x <= 1), 1, 0)\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-1, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "ra-GQVY6Pm9-"
   },
   "outputs": [],
   "source": [
    "class Softmax(activationFunction):\n",
    "    \"\"\"Normalized exponential function (softmax)\"\"\"\n",
    "    def function(self, x):\n",
    "        x  = np.subtract(x, np.max(x))        # prevent overflow\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        raise NotImplementedError(\"La derivada de Softmax no se utiliza tÃ­picamente en el entrenamiento de redes neuronales.\")\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "pNnzzwWtP2yO"
   },
   "outputs": [],
   "source": [
    "class LeakyRelu(activationFunction):\n",
    "    \"\"\"Leaky rectified linear unit function (leakyRelu)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,1e-2*x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,1e-2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "Kj60iC_mP5aR"
   },
   "outputs": [],
   "source": [
    "class ELU(activationFunction):\n",
    "    \"\"\"Exponential Linear Unit function (ELU)\"\"\"\n",
    "    def __init__(self, alpha=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "        alpha = float by default 1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,self.alpha*(np.exp(x)-1))\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,self.alpha*np.exp(x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "mpJ1CbgVP8hu"
   },
   "outputs": [],
   "source": [
    "class GELU(activationFunction):\n",
    "    \"\"\"Gaussian Error Linear Unit function (GELU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
    "    def derivative(self, x):\n",
    "        return 0.5 * (1 + erf(x / np.sqrt(2))) + (x / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "I0lFxTdgP-w9"
   },
   "outputs": [],
   "source": [
    "class PReLU(activationFunction):\n",
    "    \"\"\"Parametric rectified linear unit function (PReLU)\"\"\"\n",
    "    def __init__(self, alpha=1e-1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        alpha : float by default 1e-1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x<0,self.alpha*x,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x<0,self.alpha,1)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Zfu4-hwtQAcV"
   },
   "outputs": [],
   "source": [
    "class SELU(activationFunction):\n",
    "    \"\"\"Scaled exponential linear unit function (SELU)\"\"\"\n",
    "    def __init__(self, lamb= 1.0507, alpha=1.67326):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        lamb : float by default 1.0507\n",
    "        alpha : float by default 1.67326\n",
    "        Both are suposed to be always that value so it's recomended to not change them\n",
    "        \"\"\"\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*(np.exp(x)-1),x)\n",
    "    def derivative(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*np.exp(x),1)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "iw1z8DEkQC5j"
   },
   "outputs": [],
   "source": [
    "class SiLU(activationFunction):\n",
    "    \"\"\"Sigmoid linear unit function (SiLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return (x / (1 + np.exp(-x)))\n",
    "    def derivative(self, x):\n",
    "        return (1 + np.exp(-x) + x*np.exp(-x))/((1+np.exp(-x))**2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "CIoQz-TqQFLg"
   },
   "outputs": [],
   "source": [
    "class Softplus(activationFunction):\n",
    "    \"\"\"Smooth approximation ReLU function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    def derivative(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funciones de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunctions:\n",
    "    @staticmethod\n",
    "    def MSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def MAE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean absolute error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean absolute error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def SSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the sum of squared errors between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Sum of squared errors\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.sum((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFBX6lkR44kf"
   },
   "source": [
    "# ESTRUCTURA DE LA RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Class for the structure of a Neural Network\"\"\"\n",
    "    def __init__(self, input_size:int, layer_sizes:list[int], output_size:int, \n",
    "                 activation_funcs:list['activationFunction'], wInit:str='random',\n",
    "                 dropout_rate:float=0, regularization:str='None',lambda_reg:float=0.01)->None:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        input_size: int \n",
    "            Defines the size of the input layer\n",
    "        layer_sizes: int array \n",
    "            Defines the sizes of the ocult layers\n",
    "        output_size: int \n",
    "            Defines the size of the output layer\n",
    "        activation_funcs: activationFunction class array \n",
    "            Defines the activation function per layer\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = [input_size] + layer_sizes + [output_size]  # Incluir el tamaÃ±o de la capa de entrada y de salida\n",
    "        self.output_size = output_size\n",
    "        self.activation_funcs = activation_funcs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = len(self.layer_sizes)\n",
    "        self.weights = self._initializeWeights(wInit)\n",
    "        self.regularization = regularization\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.n_outputs = []  # Lista para almacenar las salidas antes de la funciÃ³n de activaciÃ³n\n",
    "        self.a_outputs = []\n",
    "        self.dropout_masks = []\n",
    "\n",
    "    \n",
    "    def _initializeWeights(self, wInit):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la red neuronal ya sea de manera random o mediante el metodo nguyen widraw\n",
    "        \n",
    "        :return: Lista de matrices de pesos como np.ndarrar\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        if wInit == 'random':\n",
    "            for i in range(self.num_layers - 1):\n",
    "                W = np.random.randn(self.layer_sizes[i] + 1, self.layer_sizes[i+1]) # +1 para incluir los sesgos\n",
    "                weights.append(W)\n",
    "        elif wInit == 'nguyen':\n",
    "            for i in range(self.num_layers - 1):\n",
    "                ni = self.layer_sizes[i]\n",
    "                no = self.layer_sizes[i+1]\n",
    "                g = (0.7*no) ** (1/ni)\n",
    "                active = self.activation_funcs[i].active()\n",
    "                if not np.isinf(active[0]) and not np.isinf(active[1]):\n",
    "                    W = np.random.randn(no, ni)\n",
    "                    W = W / np.linalg.norm(W, axis=1, keepdims=True)  # Normalizacion\n",
    "                    W = g * W\n",
    "                    beta = np.linspace(active[0], active[1], no).reshape(-1, 1)\n",
    "                    bias = g * (np.sign(W[:, 0]).reshape(-1, 1) * beta)\n",
    "                    W = np.hstack([W, bias])\n",
    "                    weights.append(W.T)\n",
    "                else:\n",
    "                    W = g * np.random.randn(ni+1,no)\n",
    "                    weights.append(W)\n",
    "        else:\n",
    "            raise KeyError(\"No se reconoce el inicializador\")\n",
    "        return weights\n",
    "        #return np.array(weights, dtype = object)\n",
    "    \n",
    "    def forwardPass(self, inputs, training=True):\n",
    "        A = np.hstack([inputs, np.ones((inputs.shape[0], 1))])\n",
    "        self.n_outputs = [inputs]\n",
    "        self.a_outputs = [A]\n",
    "        self.dropout_masks = []\n",
    "\n",
    "        for i, weight in enumerate(self.weights):\n",
    "            Z = np.dot(A, weight)\n",
    "            self.n_outputs.append(Z)\n",
    "            A = self.activation_funcs[i].function(Z)\n",
    "            \n",
    "            if self.dropout_rate > 0 and training and i < len(self.weights) - 1:  # Dropout en todo menos la ultima capa y solo durante el entrenamiento\n",
    "                dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, size=A.shape)\n",
    "                A *= dropout_mask\n",
    "                self.dropout_masks.append(dropout_mask)\n",
    "            elif not training and i < len(self.weights) - 1:  # Escalar datos durante la inferencia (no entrenando)\n",
    "                A *= (1 - self.dropout_rate)\n",
    "            \n",
    "            A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "            self.a_outputs.append(A)\n",
    "        return A[:, :-1]\n",
    "\n",
    "    def backwardPass(self, targets):\n",
    "        #gradients = np.array([])\n",
    "        gradients = []\n",
    "        e = targets - self.a_outputs[-1][:,:-1]\n",
    "        ge = -2*e\n",
    "        delta = ge * self.activation_funcs[-1].derivative(np.array(self.n_outputs[-1]))\n",
    "        ae = self.a_outputs[-2] #El metodo forward pass deja a_outputs aumentado\n",
    "        ge = np.dot(ae.T,delta)\n",
    "        gradients.append(ge)\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1): \n",
    "            fdx = self.activation_funcs[i].derivative(np.array(self.n_outputs[i]))\n",
    "            delta = fdx * np.dot(delta,self.weights[i][:-1].T)\n",
    "            \n",
    "            if self.dropout_rate > 0 and self.dropout_masks: #Si existe alguna mascara de dropout aplicarla\n",
    "                delta *= self.dropout_masks.pop()  # Apply dropout mask\n",
    "            \n",
    "            ae = self.a_outputs[i-1]\n",
    "            ge = np.dot(ae.T,delta)\n",
    "            gradients.insert(0,ge)\n",
    "        return gradients\n",
    "            \n",
    "    def error(self,targets,error_func):\n",
    "        \"\"\"\n",
    "        Calculate the error based on the inputs, outputs, and error function specified.\n",
    "\n",
    "        Parameters:\n",
    "        inputs: numpy.ndarray\n",
    "            Input data\n",
    "        outputs: numpy.ndarray\n",
    "            Output data\n",
    "        error_func: function\n",
    "            Error function to use (e.g., mean squared error, mean absolute error, etc.)\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Error value calculated using the specified error function.\n",
    "        \"\"\"\n",
    "        predicted_outputs = self.a_outputs[-1][:,:-1]\n",
    "        error = error_func(targets, predicted_outputs)\n",
    "        \n",
    "        if self.regularization == 'L1':\n",
    "            reg_term = self.lambda_reg * sum(np.sum(np.abs(w)) for w in self.weights) #Agregamos el termino de regularizacion para cada capa\n",
    "        elif self.regularization == 'L2':\n",
    "            reg_term = self.lambda_reg * sum(np.sum(w**2) for w in self.weights) #Agregamos el termino de regularizacion para cada capa\n",
    "        else:\n",
    "            reg_term = 0\n",
    "        return error + reg_term #Si se usa un regularizador sera el error mas el termino del mismo de lo contrario no se agregara nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"\n",
    "    Class for the optimizers based on two different algorithms\n",
    "\n",
    "    RMSProp()\n",
    "    AdamW() \n",
    "    \"\"\"\n",
    "    def __init__(self,lr:float,maxEpochs:int,goal:float,mingrad:float,nn: NeuralNetwork,\n",
    "                 inputs:np.array,targets:np.array,error_fun,show:int =1,consecutive_epochs:int =10,\n",
    "                 batch_size: int=1)->None:  \n",
    "        self.nn = nn\n",
    "        self.name = \"DEFAULT\"\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size if batch_size > 0 else inputs.shape[0]  # Si batch_size <= 0, usa todos los ejemplos\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.goal = goal\n",
    "        self.mingrad = mingrad\n",
    "        self.show = show\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.error_fun = error_fun\n",
    "        self.consecutive_epochs = consecutive_epochs\n",
    "        \n",
    "        \n",
    "    def optimize(self):\n",
    "        this = self.name\n",
    "        stop = \"\"\n",
    "        epochs = []\n",
    "        perfs  = []\n",
    "        consecutive_rise = 0  # Contador para el nÃºmero de Ã©pocas consecutivas en las que el rendimiento ha subido\n",
    "        prev_perf = float('inf')\n",
    "        num_samples = self.inputs.shape[0]  # NÃºmero de ejemplos de entrenamiento\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #Entrenamiento\n",
    "        for epoch in range(self.maxEpochs+1):\n",
    "            #Mezclar los datos\n",
    "            permutation = np.random.permutation(num_samples)\n",
    "            inputs_shuffled = self.inputs[permutation, :]\n",
    "            targets_shuffled = self.targets[permutation, :]\n",
    "\n",
    "            # Procesar mini-lotes\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_inputs = inputs_shuffled[start:end,: ]\n",
    "                batch_targets = targets_shuffled[start:end,:]\n",
    "\n",
    "                # Performance and Gradient\n",
    "                _ = self.nn.forwardPass(batch_inputs)\n",
    "                gX = self.nn.backwardPass(batch_targets)\n",
    "\n",
    "                # Aplanar y concatenar los gradientes en un solo vector\n",
    "                gX_flattened = np.concatenate([grad.flatten() for grad in gX])\n",
    "                  \n",
    "                self.train(gX_flattened)  # Pasar gX aplanado\n",
    "                \n",
    "            if self.batch_size != self.inputs.shape[0]:\n",
    "                _ = self.nn.forwardPass(self.inputs)\n",
    "                gX = self.nn.backwardPass(self.targets)\n",
    "                \n",
    "            perf = self.nn.error(self.targets, self.error_fun)\n",
    "            \n",
    "            # Aplanar y concatenar los gradientes en un solo vector\n",
    "            gX_flattened = np.concatenate([grad.flatten() for grad in gX])\n",
    "            normgX = np.linalg.norm(gX_flattened)\n",
    "\n",
    "            # Stopping criteria\n",
    "            epochs = np.append(epochs, epoch)\n",
    "            perfs = np.append(perfs, perf)\n",
    "            if np.all(perf <= self.goal):\n",
    "                stop = \"Performance goal met\"\n",
    "            elif epoch == self.maxEpochs:\n",
    "                stop = \"Maximum epoch reached, performance goal was not met\"\n",
    "            elif normgX < self.mingrad:\n",
    "                stop = \"Minimum gradient reached, performance goal was not met\"\n",
    "            elif perf >= prev_perf or (abs(perf - prev_perf) < self.goal * 10):\n",
    "                consecutive_rise += 1\n",
    "                if consecutive_rise >= self.consecutive_epochs:\n",
    "                    stop = f\"Performance has risen for {self.consecutive_epochs} consecutive epochs\"\n",
    "            elif perf < prev_perf:\n",
    "                consecutive_rise = 0\n",
    "\n",
    "            prev_perf = perf\n",
    "            if (np.fmod(epoch, self.show) == 0 or len(stop) != 0):\n",
    "                print(this, end=\": \")\n",
    "                if np.isfinite(self.maxEpochs):\n",
    "                    print(\"Epoch \", epoch, \"/\", self.maxEpochs, end=\" \")\n",
    "                if np.isfinite(self.goal):\n",
    "                    print(\", Performance %8.3e\" % perf, \"/\", self.goal, end=\" \")\n",
    "                if np.isfinite(self.mingrad):\n",
    "                    print(\", Gradient %8.3e\" % normgX, \"/\", self.mingrad)\n",
    "\n",
    "                if len(stop) != 0:\n",
    "                    print(\"\\n\", this, \":\", stop, \"\\n\")\n",
    "                    break            \n",
    "        return perfs, epochs\n",
    "\n",
    "    def train(self,gX):\n",
    "        raise NotImplementedError(\"No se ha definido el optimizador, esta es la clase base\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmsProp(Optimizer):\n",
    "    def __init__(self, nn: NeuralNetwork, inputs:np.array, targets:np.array,lr: float =1e-3, batch_size: int =0, maxEpochs: int =500, \n",
    "                 goal: float =1e-8,mingrad: float =1e-11, show:int =1, error_fun=ErrorFunctions.SSE, \n",
    "                 consecutive_epochs: int=10,WDecay:float=0,alpha:float=0.99,centered:bool=False,\n",
    "                 momentum:float=0.6,epsilon:float=1e-9) -> None:\n",
    "        if not isinstance(inputs, np.ndarray):\n",
    "            raise TypeError(\"El argumento 'inputs' debe ser un array de NumPy.\")\n",
    "        if not isinstance(targets, np.ndarray):\n",
    "            raise TypeError(\"El argumento 'targets' debe ser un array de NumPy.\")\n",
    "        super().__init__(lr,maxEpochs,goal,mingrad,nn,inputs,targets,error_fun,show,consecutive_epochs,batch_size)\n",
    "        self.name = \"trainRMSPROP\"\n",
    "        self.epsilon = epsilon\n",
    "        self.v = np.zeros_like(np.concatenate([w.flatten() for w in nn.weights]))  # Vector de acumulaciÃ³n de gradientes\n",
    "        self.vh = 0\n",
    "        self.b = 0\n",
    "        self.gAvg = 0\n",
    "        self.WDecay = WDecay\n",
    "        self.alpha = alpha\n",
    "        self.centered = centered\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def train(self, gX):        \n",
    "        if self.WDecay != 0:\n",
    "            gX = gX + gX*self.WDecay\n",
    "        self.v = self.alpha*self.v + ((1-self.alpha)*(gX**2))\n",
    "        self.vh = self.v\n",
    "        if self.centered:\n",
    "            self.gAvg = self.gAvg*self.alpha + ((1-self.alpha)*gX)\n",
    "            self.vh = self.vh - self.gAvg**2\n",
    "        if self.momentum > 0:\n",
    "            self.b = self.momentum*self.b + gX/((self.vh**(1/2))+self.epsilon)\n",
    "            update = self.lr*self.b\n",
    "        else:\n",
    "            update = self.lr*(gX/((self.vh**(1/2))+1e-8))\n",
    "                \n",
    "        # Actualizar pesos\n",
    "        start = 0\n",
    "        for i, w in enumerate(self.nn.weights):\n",
    "            shape = w.shape\n",
    "            size = np.prod(shape)\n",
    "            grad_update = update[start:start+size].reshape(shape)\n",
    "            #Aplicar la penalizacion por regularizacion si es necesario\n",
    "            if self.nn.regularization == 'L1':\n",
    "                reg_penalty = self.nn.lambda_reg * np.sign(w)\n",
    "                grad_update += reg_penalty\n",
    "            elif self.nn.regularization == 'L2':\n",
    "                reg_penalty = self.nn.lambda_reg * 2 * w\n",
    "                grad_update += reg_penalty    \n",
    "                \n",
    "            self.nn.weights[i] -= grad_update\n",
    "            start += size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printData(inputs,targets):\n",
    "    # Calcular estadÃ­sticas descriptivas para los datos de entrada\n",
    "    mean_inputs = np.mean(inputs, axis=0)\n",
    "    std_inputs = np.std(inputs, axis=0)\n",
    "    min_inputs = np.min(inputs, axis=0)\n",
    "    max_inputs = np.max(inputs, axis=0)\n",
    "\n",
    "    # Calcular estadÃ­sticas descriptivas para los datos de salida (targets)\n",
    "    mean_targets = np.mean(targets, axis=0)\n",
    "    std_targets = np.std(targets, axis=0)\n",
    "    min_targets = np.min(targets, axis=0)\n",
    "    max_targets = np.max(targets, axis=0)\n",
    "\n",
    "    # Imprimir las estadÃ­sticas\n",
    "    print(\"EstadÃ­sticas de los datos de entrada:\")\n",
    "    print(\"Media:\", mean_inputs)\n",
    "    print(\"DesviaciÃ³n estÃ¡ndar:\", std_inputs)\n",
    "    print(\"MÃ­nimo:\", min_inputs)\n",
    "    print(\"MÃ¡ximo:\", max_inputs)\n",
    "\n",
    "    print(\"\\nEstadÃ­sticas de los datos de salida:\")\n",
    "    print(\"Media:\", mean_targets)\n",
    "    print(\"DesviaciÃ³n estÃ¡ndar:\", std_targets)\n",
    "    print(\"MÃ­nimo:\", min_targets)\n",
    "    print(\"MÃ¡ximo:\", max_targets)\n",
    "    \n",
    "def min_max(inputs,targets):\n",
    "    inputs_min = np.min(inputs, axis=0, keepdims=True)\n",
    "    inputs_max = np.max(inputs, axis=0, keepdims=True)\n",
    "    inputs_normalized = (inputs - inputs_min) / (inputs_max - inputs_min)\n",
    "    \n",
    "    targets_min = np.min(targets, axis=0, keepdims=True)\n",
    "    targets_max = np.max(targets, axis=0, keepdims=True)\n",
    "    targets_normalized = (targets - targets_min) / (targets_max - targets_min)\n",
    "    return inputs_normalized,targets_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    activationFunction()\n",
    "    neural_network = NeuralNetwork(input_size = 2,\n",
    "                                layer_sizes = [2],\n",
    "                                output_size = 1,\n",
    "                                activation_funcs = [Swish(),Logsig()],\n",
    "                                wInit = 'nguyen')\n",
    "\n",
    "    # Carga el archivo .mat\n",
    "    inputs = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "    targets =np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "    for weight in neural_network.weights:\n",
    "        print(f\"Tamano de los pesos: {weight.shape}\")\n",
    "    \n",
    "    Optimizador = RmsProp(nn=neural_network,\n",
    "                          inputs=inputs,\n",
    "                          targets=targets,\n",
    "                          lr=1e-2,\n",
    "                          maxEpochs=500,\n",
    "                          show=50,\n",
    "                          consecutive_epochs=20,\n",
    "                          goal=1e-4,\n",
    "                          mingrad=1e-8,\n",
    "                          batch_size=1,\n",
    "                          error_fun=ErrorFunctions.MSE)\n",
    "    \n",
    "    perfs,epochs = Optimizador.optimize()\n",
    "    \"\"\"\n",
    "    \n",
    "    activationFunction()\n",
    "    neural_network = NeuralNetwork(input_size = 2,\n",
    "                                layer_sizes = [30,30,30],\n",
    "                                output_size = 2,\n",
    "                                activation_funcs = [Relu(),Relu(),Relu(),Purelin()],\n",
    "                                wInit = 'nguyen',\n",
    "                                dropout_rate=0.05,\n",
    "                                regularization='L2')\n",
    "\n",
    "    # Carga el archivo .mat\n",
    "    data = loadmat('engine_dataset.mat')\n",
    "    inputs = data['engineInputs'].T\n",
    "    targets = data['engineTargets'].T\n",
    "    \n",
    "    printData(inputs,targets)\n",
    "    inputs,targets = min_max(inputs,targets)\n",
    "    print(\"===========================\")\n",
    "    printData(inputs,targets)\n",
    "    \n",
    "    Optimizador = RmsProp(nn=neural_network,\n",
    "                          inputs=inputs,\n",
    "                          targets=targets,\n",
    "                          lr=1e-3,\n",
    "                          maxEpochs=2000,\n",
    "                          show=200,\n",
    "                          consecutive_epochs=10,\n",
    "                          mingrad=1e-8,\n",
    "                          batch_size=250,\n",
    "                          error_fun=ErrorFunctions.MSE)\n",
    "    \n",
    "    perfs,epochs = Optimizador.optimize()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, perfs)\n",
    "    plt.title('Performance')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(perfs)\n",
    "    \"\"\"     \n",
    "    outputs = neural_network.forwardPass(inputs,False)\n",
    "    print(f\"Salida:{outputs}\")\n",
    "    print(\"PredicciÃ³n binaria:\")\n",
    "    for fila in outputs.T:  # Iterar sobre las filas de la matriz de salida\n",
    "        for valor in fila:\n",
    "            if valor > 0.5:\n",
    "                print(\"1\", end=\" \")\n",
    "            else:\n",
    "                print(\"0\", end=\" \")\n",
    "    resultado_esperado = [0, 1, 1, 0]\n",
    "    print(\"\\nResultado esperado:\\n \", resultado_esperado)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EstadÃ­sticas de los datos de entrada:\n",
      "Media: [ 141.23511259 1259.53911593]\n",
      "DesviaciÃ³n estÃ¡ndar: [ 90.67771106 354.62917678]\n",
      "MÃ­nimo: [  0.6 576.2]\n",
      "MÃ¡ximo: [ 314.  1801.8]\n",
      "\n",
      "EstadÃ­sticas de los datos de salida:\n",
      "Media: [754.19974979 961.74895746]\n",
      "DesviaciÃ³n estÃ¡ndar: [548.43987051 465.9340708 ]\n",
      "MÃ­nimo: [-176.7    0. ]\n",
      "MÃ¡ximo: [1784.3 1774. ]\n",
      "===========================\n",
      "EstadÃ­sticas de los datos de entrada:\n",
      "Media: [0.44873999 0.55755476]\n",
      "DesviaciÃ³n estÃ¡ndar: [0.28933539 0.28935148]\n",
      "MÃ­nimo: [0. 0.]\n",
      "MÃ¡ximo: [1. 1.]\n",
      "\n",
      "EstadÃ­sticas de los datos de salida:\n",
      "Media: [0.47470665 0.54213583]\n",
      "DesviaciÃ³n estÃ¡ndar: [0.27967357 0.26264604]\n",
      "MÃ­nimo: [0. 0.]\n",
      "MÃ¡ximo: [1. 1.]\n",
      "\n",
      "\n",
      "trainRMSPROP: Epoch  0 / 2000 , Performance 1.347e+04 / 1e-08 , Gradient 1.159e+08 / 1e-08\n",
      "trainRMSPROP: Epoch  200 / 2000 , Performance 2.486e-01 / 1e-08 , Gradient 1.418e+03 / 1e-08\n",
      "trainRMSPROP: Epoch  400 / 2000 , Performance 1.111e-01 / 1e-08 , Gradient 5.834e+02 / 1e-08\n",
      "trainRMSPROP: Epoch  600 / 2000 , Performance 1.048e-01 / 1e-08 , Gradient 1.431e+02 / 1e-08\n",
      "trainRMSPROP: Epoch  602 / 2000 , Performance 1.072e-01 / 1e-08 , Gradient 1.621e+02 / 1e-08\n",
      "\n",
      " trainRMSPROP : Performance has risen for 10 consecutive epochs \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.34711633e+04 3.15244411e+03 1.38536285e+03 6.13106229e+02\n",
      " 2.63047811e+02 1.27933090e+02 5.77176279e+01 2.90362746e+01\n",
      " 1.52332085e+01 9.45378716e+00 6.20092331e+00 4.45182598e+00\n",
      " 3.42844711e+00 2.77159904e+00 2.28957901e+00 1.93128482e+00\n",
      " 1.51163932e+00 1.19846231e+00 1.01100914e+00 8.77327315e-01\n",
      " 7.71463523e-01 6.87624719e-01 6.28114106e-01 5.80283755e-01\n",
      " 5.47519989e-01 5.11071881e-01 4.64441134e-01 4.32608973e-01\n",
      " 4.14914112e-01 4.02847670e-01 3.98380836e-01 3.91622702e-01\n",
      " 3.87532055e-01 3.84181133e-01 3.79486651e-01 3.75125855e-01\n",
      " 3.70264454e-01 3.65751251e-01 3.61199066e-01 3.56650869e-01\n",
      " 3.52398960e-01 3.48354124e-01 3.44650123e-01 3.41179105e-01\n",
      " 3.38007641e-01 3.35125111e-01 3.32509452e-01 3.30172509e-01\n",
      " 3.28040883e-01 3.26120234e-01 3.24384719e-01 3.22824670e-01\n",
      " 3.21382963e-01 3.20087058e-01 3.18913467e-01 3.17822109e-01\n",
      " 3.16804407e-01 3.15880176e-01 3.15029743e-01 3.14178478e-01\n",
      " 3.13408428e-01 3.12674906e-01 3.11977366e-01 3.11300416e-01\n",
      " 3.10660572e-01 3.10025665e-01 3.09409298e-01 3.08811090e-01\n",
      " 3.08225279e-01 3.07676807e-01 3.07129415e-01 3.06588344e-01\n",
      " 3.06039342e-01 3.05514566e-01 3.04992272e-01 3.04462617e-01\n",
      " 3.03959247e-01 3.03453934e-01 3.02941013e-01 3.02417933e-01\n",
      " 3.01923989e-01 3.01434601e-01 3.00952826e-01 3.00458215e-01\n",
      " 2.99950641e-01 2.99469997e-01 2.98984591e-01 2.98506781e-01\n",
      " 2.98001523e-01 2.97550168e-01 2.97056157e-01 2.96609023e-01\n",
      " 2.96121591e-01 2.95669475e-01 2.95184461e-01 2.94722627e-01\n",
      " 2.94247276e-01 2.93810234e-01 2.93329637e-01 2.92886642e-01\n",
      " 2.92436824e-01 2.92009565e-01 2.91555343e-01 2.91100090e-01\n",
      " 2.90683778e-01 2.90207422e-01 2.89808938e-01 2.89368808e-01\n",
      " 2.88947920e-01 2.88524012e-01 2.88084454e-01 2.87668056e-01\n",
      " 2.87270801e-01 2.86841809e-01 2.86458777e-01 2.86042350e-01\n",
      " 2.85644262e-01 2.85230607e-01 2.84842199e-01 2.84452589e-01\n",
      " 2.84046577e-01 2.83634837e-01 2.83241537e-01 2.82872282e-01\n",
      " 2.82473492e-01 2.82095794e-01 2.81700080e-01 2.81313529e-01\n",
      " 2.80918333e-01 2.80546074e-01 2.80136790e-01 2.79781661e-01\n",
      " 2.79391054e-01 2.78988582e-01 2.78630460e-01 2.78245078e-01\n",
      " 2.77830374e-01 2.77503963e-01 2.77074067e-01 2.76706841e-01\n",
      " 2.76344270e-01 2.75930369e-01 2.75538617e-01 2.75140746e-01\n",
      " 2.74785225e-01 2.74389495e-01 2.73947034e-01 2.73580613e-01\n",
      " 2.73163683e-01 2.72802387e-01 2.72346089e-01 2.71961259e-01\n",
      " 2.71566125e-01 2.71138776e-01 2.70724571e-01 2.70320141e-01\n",
      " 2.69895796e-01 2.69465765e-01 2.69077608e-01 2.68604467e-01\n",
      " 2.68197690e-01 2.67728405e-01 2.67320907e-01 2.66873450e-01\n",
      " 2.66433665e-01 2.65977365e-01 2.65521082e-01 2.65082001e-01\n",
      " 2.64597225e-01 2.64161231e-01 2.63666859e-01 2.63198334e-01\n",
      " 2.62754296e-01 2.62283134e-01 2.61765147e-01 2.61320635e-01\n",
      " 2.60810330e-01 2.60305749e-01 2.59796954e-01 2.59331029e-01\n",
      " 2.58822479e-01 2.58298995e-01 2.57823887e-01 2.57282047e-01\n",
      " 2.56772466e-01 2.56278683e-01 2.55755504e-01 2.55251802e-01\n",
      " 2.54758283e-01 2.54224008e-01 2.53715245e-01 2.53160697e-01\n",
      " 2.52688172e-01 2.52156214e-01 2.51645079e-01 2.51122978e-01\n",
      " 2.50563483e-01 2.50094041e-01 2.49569207e-01 2.49053344e-01\n",
      " 2.48567014e-01 2.48011082e-01 2.47547368e-01 2.47023335e-01\n",
      " 2.46496768e-01 2.46024955e-01 2.45529664e-01 2.45008977e-01\n",
      " 2.44527923e-01 2.44028922e-01 2.43615015e-01 2.43122504e-01\n",
      " 2.42663961e-01 2.42203341e-01 2.41782352e-01 2.41315334e-01\n",
      " 2.40853797e-01 2.40493316e-01 2.39968267e-01 2.39553509e-01\n",
      " 2.39135367e-01 2.38743270e-01 2.38284112e-01 2.37883813e-01\n",
      " 2.37464253e-01 2.37064367e-01 2.36697592e-01 2.36310680e-01\n",
      " 2.35945409e-01 2.35609813e-01 2.35156140e-01 2.34880147e-01\n",
      " 2.34563864e-01 2.34184808e-01 2.33898893e-01 2.33540074e-01\n",
      " 2.33318939e-01 2.32970495e-01 2.32643859e-01 2.32333803e-01\n",
      " 2.32065488e-01 2.31814570e-01 2.31515196e-01 2.31181926e-01\n",
      " 2.30962056e-01 2.30761497e-01 2.30480703e-01 2.30260018e-01\n",
      " 2.29988728e-01 2.29844423e-01 2.29547508e-01 2.29336399e-01\n",
      " 2.29169807e-01 2.28926803e-01 2.28704119e-01 2.28570379e-01\n",
      " 2.28350161e-01 2.28224668e-01 2.28056605e-01 2.27880504e-01\n",
      " 2.27758555e-01 2.27520723e-01 2.27402117e-01 2.27261642e-01\n",
      " 2.27043254e-01 2.26884373e-01 2.26800983e-01 2.26614989e-01\n",
      " 2.26560520e-01 2.26357410e-01 2.26256689e-01 2.26121336e-01\n",
      " 2.26062969e-01 2.25927123e-01 2.25819477e-01 2.25664267e-01\n",
      " 2.25592942e-01 2.25460769e-01 2.25383270e-01 2.25349659e-01\n",
      " 2.25232676e-01 2.25176602e-01 2.25051884e-01 2.25027109e-01\n",
      " 2.24885782e-01 2.24798031e-01 2.24743282e-01 2.24707643e-01\n",
      " 2.24606565e-01 2.24533032e-01 2.24485384e-01 2.24371810e-01\n",
      " 2.24334295e-01 2.24248243e-01 2.24188666e-01 2.24152352e-01\n",
      " 2.24122873e-01 2.24066772e-01 2.23981216e-01 2.23911598e-01\n",
      " 2.23824001e-01 2.23682996e-01 2.23561787e-01 2.23082406e-01\n",
      " 2.22281401e-01 2.20735349e-01 2.18440840e-01 2.14774406e-01\n",
      " 2.09913417e-01 2.03414782e-01 1.94255846e-01 1.82450669e-01\n",
      " 1.70367834e-01 1.58900126e-01 1.49552235e-01 1.42390146e-01\n",
      " 1.36885735e-01 1.34200224e-01 1.32164539e-01 1.29603504e-01\n",
      " 1.28335770e-01 1.26853542e-01 1.25497961e-01 1.23827872e-01\n",
      " 1.22127776e-01 1.21127988e-01 1.20152160e-01 1.20182592e-01\n",
      " 1.18710474e-01 1.18714486e-01 1.17239002e-01 1.19188337e-01\n",
      " 1.18727462e-01 1.20039043e-01 1.20403426e-01 1.19544236e-01\n",
      " 1.19655703e-01 1.20081629e-01 1.22592359e-01 1.22536948e-01\n",
      " 1.22027461e-01 1.22853508e-01 1.23510151e-01 1.23739035e-01\n",
      " 1.26062109e-01 1.26601377e-01 1.26619725e-01 1.28354020e-01\n",
      " 1.27094125e-01 1.28909865e-01 1.28308707e-01 1.28672118e-01\n",
      " 1.28430015e-01 1.28316573e-01 1.30022881e-01 1.30762147e-01\n",
      " 1.30397909e-01 1.31882244e-01 1.33323241e-01 1.34301437e-01\n",
      " 1.34681980e-01 1.34349658e-01 1.34677758e-01 1.33893118e-01\n",
      " 1.33480528e-01 1.33973163e-01 1.33725589e-01 1.33616328e-01\n",
      " 1.31972223e-01 1.31795042e-01 1.30147990e-01 1.30433032e-01\n",
      " 1.28872601e-01 1.26692152e-01 1.27346375e-01 1.26527501e-01\n",
      " 1.24926575e-01 1.24097737e-01 1.23551310e-01 1.23465255e-01\n",
      " 1.21657888e-01 1.20053274e-01 1.20907428e-01 1.20259106e-01\n",
      " 1.18897414e-01 1.18437081e-01 1.19718204e-01 1.17516144e-01\n",
      " 1.17041842e-01 1.16820040e-01 1.16797038e-01 1.16087939e-01\n",
      " 1.15887633e-01 1.15258103e-01 1.14360067e-01 1.13575911e-01\n",
      " 1.13558802e-01 1.13402431e-01 1.12377960e-01 1.12027660e-01\n",
      " 1.11127404e-01 1.11400082e-01 1.11115937e-01 1.09363715e-01\n",
      " 1.09786239e-01 1.09203326e-01 1.08077919e-01 1.07755229e-01\n",
      " 1.08520575e-01 1.08305947e-01 1.06977004e-01 1.06102664e-01\n",
      " 1.06029430e-01 1.05001466e-01 1.05035194e-01 1.04939812e-01\n",
      " 1.04233452e-01 1.04269998e-01 1.04095708e-01 1.02758075e-01\n",
      " 1.02938357e-01 1.03312062e-01 1.02541026e-01 1.02138602e-01\n",
      " 1.02238763e-01 1.02964716e-01 1.02085028e-01 1.01709301e-01\n",
      " 1.02086926e-01 1.01548013e-01 1.00932482e-01 9.98860093e-02\n",
      " 1.00673734e-01 1.00621634e-01 9.95104121e-02 9.91732969e-02\n",
      " 9.87355746e-02 9.91444845e-02 9.77536573e-02 9.82767643e-02\n",
      " 9.65551186e-02 9.73872755e-02 9.63555292e-02 9.75584775e-02\n",
      " 9.64996437e-02 9.55612616e-02 9.55173697e-02 9.60835896e-02\n",
      " 9.48131398e-02 9.40848645e-02 9.46366393e-02 9.39276625e-02\n",
      " 9.35221701e-02 9.45180949e-02 9.47136968e-02 9.41294071e-02\n",
      " 9.30371329e-02 9.44607562e-02 9.37801599e-02 9.49875013e-02\n",
      " 9.39613278e-02 9.55624344e-02 9.48915798e-02 9.55065340e-02\n",
      " 9.59765114e-02 9.56081282e-02 9.63310082e-02 9.58117637e-02\n",
      " 9.46033751e-02 9.57615115e-02 9.59231516e-02 9.64995991e-02\n",
      " 9.51558730e-02 9.61810730e-02 9.51082390e-02 9.48050717e-02\n",
      " 9.55646796e-02 9.61607490e-02 9.56492981e-02 9.64339620e-02\n",
      " 9.62517365e-02 9.65528761e-02 9.65783657e-02 9.60760383e-02\n",
      " 9.55850623e-02 9.65529851e-02 9.61162020e-02 9.49155385e-02\n",
      " 9.56982264e-02 9.51970473e-02 9.49985563e-02 9.53651586e-02\n",
      " 9.59650886e-02 9.72328430e-02 9.79297246e-02 9.76580245e-02\n",
      " 9.84753242e-02 9.84235644e-02 9.81211103e-02 9.86670486e-02\n",
      " 9.84177253e-02 9.74122223e-02 9.71757445e-02 9.81638683e-02\n",
      " 9.70410467e-02 9.69104165e-02 9.73569517e-02 9.67883046e-02\n",
      " 9.72309607e-02 9.69521560e-02 9.67368435e-02 9.77273003e-02\n",
      " 9.65907609e-02 9.66103990e-02 9.67908200e-02 9.60216858e-02\n",
      " 9.65073898e-02 9.52791459e-02 9.68609096e-02 9.53284554e-02\n",
      " 9.53675384e-02 9.58535214e-02 9.48513535e-02 9.50898958e-02\n",
      " 9.51926901e-02 9.47186596e-02 9.53732455e-02 9.52198645e-02\n",
      " 9.50381213e-02 9.52790579e-02 9.54338242e-02 9.50625812e-02\n",
      " 9.52526915e-02 9.50718344e-02 9.51398304e-02 9.49469548e-02\n",
      " 9.66942012e-02 9.54271044e-02 9.59078188e-02 9.60279108e-02\n",
      " 9.62291878e-02 9.62190575e-02 9.69302922e-02 9.72261104e-02\n",
      " 9.80567366e-02 9.80259189e-02 9.78882591e-02 9.87095833e-02\n",
      " 9.94095863e-02 9.99713194e-02 9.95175622e-02 9.98598256e-02\n",
      " 1.00700616e-01 1.00470109e-01 1.02034034e-01 1.01506116e-01\n",
      " 1.01074953e-01 1.01669106e-01 1.01783703e-01 1.01812910e-01\n",
      " 1.02236417e-01 1.02729338e-01 1.01851151e-01 1.02129601e-01\n",
      " 1.02209530e-01 1.02356113e-01 1.01715747e-01 1.02242248e-01\n",
      " 1.02484011e-01 1.01871770e-01 1.02658885e-01 1.02838595e-01\n",
      " 1.03210268e-01 1.03172175e-01 1.03465499e-01 1.04639352e-01\n",
      " 1.04058159e-01 1.04359574e-01 1.03841064e-01 1.04572967e-01\n",
      " 1.03305911e-01 1.03731723e-01 1.03014089e-01 1.02451225e-01\n",
      " 1.02779623e-01 1.02569280e-01 1.02363571e-01 1.01550164e-01\n",
      " 1.01762240e-01 1.01669799e-01 1.01648854e-01 1.01873749e-01\n",
      " 1.01117599e-01 1.01367416e-01 1.01813675e-01 1.01893540e-01\n",
      " 1.02420656e-01 1.02810270e-01 1.03282828e-01 1.04415699e-01\n",
      " 1.04821328e-01 1.05799481e-01 1.07224581e-01]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
