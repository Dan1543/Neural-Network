{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "id": "3JDc_vCt4Lkj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1ATJTpO4zb_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FUNCIONES DE ACTIVACION Y SUS DERIVADAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g19AzOYTXYc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clases de cada funcion (contiene su funcion y su derivada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "id": "AmUUjdrl4qSr"
   },
   "outputs": [],
   "source": [
    "class activationFunction:\n",
    "    \"\"\"\n",
    "    Class for activation functions meant to be used on Neural Networks\n",
    "    All the activation functions and its derivatives are available on the subclases\n",
    "    Subclasses:\n",
    "    Swish()\n",
    "    Relu()\n",
    "    Purelin()\n",
    "    Logsig()\n",
    "    Tansig()\n",
    "    Radbas()\n",
    "    Tribas()\n",
    "    RadBasN()\n",
    "    HardLim()\n",
    "    HardLims()\n",
    "    SatLin()\n",
    "    SatLins()\n",
    "    Softmax()\n",
    "    LeakyRelu()\n",
    "    ELU()\n",
    "    GELU()\n",
    "    PReLU()\n",
    "    SELU()\n",
    "    SiLU()\n",
    "    Softplus()\n",
    "    \"\"\"\n",
    "    def function(self,x):\n",
    "        \"\"\"Activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def derivative(self,x):\n",
    "        \"\"\"Derivative of the activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def active(self):\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "id": "vl52Q1Rl4sbo"
   },
   "outputs": [],
   "source": [
    "class Swish(activationFunction):\n",
    "    \"\"\"Scaled Exponential Linear Unit With a Shift function\"\"\"\n",
    "    def __init__(self, beta=1):\n",
    "        self.beta = beta\n",
    "    def function(self, x):\n",
    "        return x * (1 / (1 + np.exp(-self.beta * x)))\n",
    "    def derivative(self, x):\n",
    "        return (self.beta * self.function(x)) + (1 / (1 + np.exp(-self.beta * x))) * (1 - self.beta * self.function(x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "id": "8shE1T3W4yJz"
   },
   "outputs": [],
   "source": [
    "class Relu(activationFunction):\n",
    "    \"\"\"Rectified linear unit function (ReLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "id": "j2v-Y_2495Yb"
   },
   "outputs": [],
   "source": [
    "class Purelin(activationFunction):\n",
    "    \"\"\"Linear (Identity) function\"\"\"\n",
    "    def function(self,x):\n",
    "      return x\n",
    "    def derivative(self,x):\n",
    "      return np.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "id": "KuoUYoUuIG6P"
   },
   "outputs": [],
   "source": [
    "class Logsig(activationFunction):\n",
    "    \"\"\"Logistic function\"\"\"\n",
    "    def function(self, x):\n",
    "      return 1 / (1 + np.exp(-x))\n",
    "    def derivative(self,x):\n",
    "        return self.function(x) * (1 - self.function(x))\n",
    "    def active(self):\n",
    "        out = [-4.0, 4.0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "id": "FbhNfFvmI9Cd"
   },
   "outputs": [],
   "source": [
    "class Tansig(activationFunction):\n",
    "    \"\"\"Hyperbolic function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.tanh(x)\n",
    "    def derivative(self,x):\n",
    "        return  1- np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "id": "jIZd2WWFKioZ"
   },
   "outputs": [],
   "source": [
    "class Radbas(activationFunction):\n",
    "    \"\"\"Gaussian function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.exp(-x**2)\n",
    "    def derivative(self,x):\n",
    "        return -2 * x * np.exp(-x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "id": "UTE_IxFnLIkU"
   },
   "outputs": [],
   "source": [
    "class Tribas(activationFunction):\n",
    "    \"\"\"Triangular basis function\"\"\"\n",
    "    def function(self, x):\n",
    "      return np.maximum(0, 1 - np.abs(x))\n",
    "    def derivative(self, x):\n",
    "      return np.where(np.abs(x) < 1, -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "id": "lgC1nhgnLjeX"
   },
   "outputs": [],
   "source": [
    "class RadBasN(activationFunction):\n",
    "    \"\"\"Normalized radial basis function\"\"\"\n",
    "    def __init__(self, sigma=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        sigma : float by default 1\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def function(self, x):\n",
    "      return np.exp(-0.5 * (x / self.sigma)**2)\n",
    "\n",
    "    def derivative(self, x):\n",
    "      return -x / self.sigma**2 * np.exp(-0.5 * (x / self.sigma)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "id": "yhHg-ZxGNLkp"
   },
   "outputs": [],
   "source": [
    "class HardLim(activationFunction):\n",
    "    \"\"\"Hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "id": "Vx0uAWHENjcZ"
   },
   "outputs": [],
   "source": [
    "class HardLims(activationFunction):\n",
    "    \"\"\"Symmetric hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "id": "s44mofomN2ju"
   },
   "outputs": [],
   "source": [
    "class SatLin(activationFunction):\n",
    "    \"\"\"Saturatin linear function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, 0, None)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(x >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "id": "3W5XynwgN-wb"
   },
   "outputs": [],
   "source": [
    "class SatLins(activationFunction):\n",
    "    \"\"\"Symmetric saturating function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, -1, 1)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(np.logical_and(x >= -1, x <= 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "id": "ra-GQVY6Pm9-"
   },
   "outputs": [],
   "source": [
    "class Softmax(activationFunction):\n",
    "    \"\"\"Normalized exponential function (softmax)\"\"\"\n",
    "    def function(self, x):\n",
    "        exps = np.exp(x)\n",
    "        sums = np.sum(exps)\n",
    "        return np.divide(exps, sums)\n",
    "    def derivative(self, x):\n",
    "        raise NotImplementedError(\"La derivada de Softmax no se utiliza típicamente en el entrenamiento de redes neuronales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "id": "pNnzzwWtP2yO"
   },
   "outputs": [],
   "source": [
    "class LeakyRelu(activationFunction):\n",
    "    \"\"\"Leaky rectified linear unit function (leakyRelu)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,1e-2*x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "id": "Kj60iC_mP5aR"
   },
   "outputs": [],
   "source": [
    "class ELU(activationFunction):\n",
    "    \"\"\"Exponential Linear Unit function (ELU)\"\"\"\n",
    "    def __init__(self, alpha=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "        alpha = float by default 1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,self.alpha*(np.exp(x)-1))\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,self.alpha*np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "id": "mpJ1CbgVP8hu"
   },
   "outputs": [],
   "source": [
    "class GELU(activationFunction):\n",
    "    \"\"\"Gaussian Error Linear Unit function (GELU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
    "    def derivative(self, x):\n",
    "        return 0.5 * (1 + erf(x / np.sqrt(2))) + (x / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "id": "I0lFxTdgP-w9"
   },
   "outputs": [],
   "source": [
    "class PReLU(activationFunction):\n",
    "    \"\"\"Parametric rectified linear unit function (PReLU)\"\"\"\n",
    "    def __init__(self, alpha=1e-1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        alpha : float by default 1e-1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x<0,self.alpha*x,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x<0,self.alpha,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "id": "Zfu4-hwtQAcV"
   },
   "outputs": [],
   "source": [
    "class SELU(activationFunction):\n",
    "    \"\"\"Scaled exponential linear unit function (SELU)\"\"\"\n",
    "    def __init__(self, lamb= 1.0507, alpha=1.67326):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        lamb : float by default 1.0507\n",
    "        alpha : float by default 1.67326\n",
    "        Both are suposed to be always that value so it's recomended to not change them\n",
    "        \"\"\"\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*(np.exp(x)-1),x)\n",
    "    def derivative(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*np.exp(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "id": "iw1z8DEkQC5j"
   },
   "outputs": [],
   "source": [
    "class SiLU(activationFunction):\n",
    "    \"\"\"Sigmoid linear unit function (SiLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return (x / (1 + np.exp(-x)))\n",
    "    def derivative(self, x):\n",
    "        return (1 + np.exp(-x) + x*np.exp(-x))/((1+np.exp(-x))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "id": "CIoQz-TqQFLg"
   },
   "outputs": [],
   "source": [
    "class Softplus(activationFunction):\n",
    "    \"\"\"Smooth approximation ReLU function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    def derivative(self, x):\n",
    "        return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funciones de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunctions:\n",
    "    @staticmethod\n",
    "    def MSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def MAE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean absolute error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean absolute error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def SSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the sum of squared errors between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Sum of squared errors\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.sum((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFBX6lkR44kf"
   },
   "source": [
    "# ESTRUCTURA DE LA RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Class for the structure of a Neural Network\"\"\"\n",
    "    def __init__(self, input_size, layer_sizes, output_size, activation_funcs, wInit='random'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        input_size: int \n",
    "            Defines the size of the input layer\n",
    "        layer_sizes: int array \n",
    "            Defines the sizes of the ocult layers\n",
    "        output_size: int \n",
    "            Defines the size of the output layer\n",
    "        activation_funcs: activationFunction class array \n",
    "            Defines the activation function per layer\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = [input_size] + layer_sizes + [output_size]  # Incluir el tamaño de la capa de entrada y de salida\n",
    "        self.output_size = output_size\n",
    "        self.activation_funcs = activation_funcs\n",
    "\n",
    "        self.num_layers = len(self.layer_sizes)\n",
    "        self.weights = self._initializeWeights(wInit)\n",
    "        self.n_outputs = []  # Lista para almacenar las salidas antes de la función de activación\n",
    "        self.a_outputs = []\n",
    "    \n",
    "    def _initializeWeights(self, wInit):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la red neuronal ya sea de manera random o mediante el metodo nguyen widraw\n",
    "        \n",
    "        :return: Lista de matrices de pesos como np.ndarrar\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        if wInit == 'random':\n",
    "            for i in range(self.num_layers - 1):\n",
    "                weights_matrix = np.random.randn(self.layer_sizes[i] + 1, self.layer_sizes[i+1]) # +1 para incluir los sesgos\n",
    "                weights.append(weights_matrix)\n",
    "        elif wInit == 'nguyen':\n",
    "            raise NotImplementedError(\"La inicializacion mediante nguyen-widraw se implementara en futuras actualizaciones\")\n",
    "        else:\n",
    "            raise KeyError(\"No se reconoce el inicializador\")\n",
    "        return np.array(weights, dtype = object)\n",
    "    \n",
    "    def forwardPass(self, inputs):\n",
    "        A = np.hstack([inputs,np.ones((inputs.shape[0], 1))])\n",
    "        self.n_outputs = [inputs] #La primera n siempre es igual a los inputs\n",
    "        self.a_outputs = [A]\n",
    "        # Iterar sobre cada capa de la red\n",
    "        for weight in self.weights:\n",
    "            # Cálculo del producto punto entre los pesos y el input aumentado por el sesgo\n",
    "            Z = np.dot(A,weight)\n",
    "            self.n_outputs.append(Z)\n",
    "            # Aplicación de la función de activación correspondiente\n",
    "            A = self.activation_funcs[len(self.a_outputs)-1].function(Z)\n",
    "            A = np.hstack([A,np.ones((A.shape[0], 1))])\n",
    "            self.a_outputs.append(A)  \n",
    "        return A[:,:-1] # TODO MENOS LA COLUMNA AUMENTADA\n",
    "\n",
    "    def backwardPass(self, targets):\n",
    "        #gradients = np.array([])\n",
    "        gradients = []\n",
    "        e = targets - self.a_outputs[-1][:,:-1]\n",
    "        ge = -2*e\n",
    "        delta = ge * self.activation_funcs[-1].derivative(np.array(self.n_outputs[-1]))\n",
    "        ae = self.a_outputs[-2] #El metodo forward pass deja a_outputs aumentado\n",
    "        ge = np.dot(ae.T,delta)\n",
    "        #ge = delta * ae.T\n",
    "        #gradients = np.concatenate((ge.flatten(),gradients))\n",
    "        gradients.append(ge)\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1): \n",
    "            fdx = self.activation_funcs[i].derivative(np.array(self.n_outputs[i]))\n",
    "            delta = fdx * np.dot(delta,self.weights[i][:-1].T)\n",
    "            ae = self.a_outputs[i-1]\n",
    "            ge = np.dot(ae.T,delta)\n",
    "            gradients.insert(0,ge)\n",
    "            #gradients = np.concatenate((ge.flatten(),gradients))            \n",
    "        return gradients\n",
    "            \n",
    "    def error(self,targets,error_func):\n",
    "        \"\"\"\n",
    "        Calculate the error based on the inputs, outputs, and error function specified.\n",
    "\n",
    "        Parameters:\n",
    "        inputs: numpy.ndarray\n",
    "            Input data\n",
    "        outputs: numpy.ndarray\n",
    "            Output data\n",
    "        error_func: function\n",
    "            Error function to use (e.g., mean squared error, mean absolute error, etc.)\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Error value calculated using the specified error function.\n",
    "        \"\"\"\n",
    "        predicted_outputs = self.a_outputs[-1][:,:-1]\n",
    "        return error_func(targets, predicted_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"\n",
    "    Class for the optimizers based on two different algorithms\n",
    "\n",
    "    RMSProp()\n",
    "    AdamW() \n",
    "    \"\"\"\n",
    "    def __init__(self,lr:float,maxEpochs:int,goal:float,mingrad:float,nn: NeuralNetwork,\n",
    "                 inputs,targets,error_fun,show:int =1,consecutive_epochs:int =10,\n",
    "                 num_batch: int=1)->None:  \n",
    "        self.nn = nn\n",
    "        self.name = \"DEFAULT\"\n",
    "        self.lr = lr\n",
    "        self.num_batch = num_batch\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.goal = goal\n",
    "        self.mingrad = mingrad\n",
    "        self.show = show\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.error_fun = error_fun\n",
    "        self.consecutive_epochs = consecutive_epochs\n",
    "        \n",
    "        \n",
    "    def optimize(self):\n",
    "        this = self.name\n",
    "        stop = \"\"\n",
    "        epochs = []\n",
    "        perfs  = []\n",
    "        consecutive_rise = 0  # Contador para el número de épocas consecutivas en las que el rendimiento ha subido\n",
    "        prev_perf = float('inf')\n",
    "        print(\"\\n\")\n",
    "        # Train\n",
    "        for epoch in range(self.maxEpochs+1):\n",
    "            # Performance and Gradient\n",
    "            _ = self.nn.forwardPass(self.inputs)\n",
    "            gX = self.nn.backwardPass(self.targets)\n",
    "            perf = self.nn.error(self.targets, self.error_fun)            \n",
    "            \n",
    "            # Aplanar y concatenar los gradientes en un solo vector\n",
    "            gX_flattened = np.concatenate([grad.flatten() for grad in gX])\n",
    "            normgX = np.linalg.norm(gX_flattened)  \n",
    "            #normgX = np.linalg.norm(gX)  \n",
    "            \n",
    "            # Stopping criteria\n",
    "            if np.all(perf <= self.goal):\n",
    "                stop = \"Performance goal met\"\n",
    "            elif epoch == self.maxEpochs:\n",
    "                stop = \"Maximum epoch reached, performance goal was not met\"\n",
    "            elif normgX < self.mingrad:\n",
    "                stop = \"Minimum gradient reached, performance goal was not met\"\n",
    "            elif perf >= prev_perf:\n",
    "                consecutive_rise += 1\n",
    "                if consecutive_rise >= self.consecutive_epochs:\n",
    "                    stop = f\"Performance has risen for {self.consecutive_epochs} consecutive epochs\"\n",
    "            elif perf < prev_perf:\n",
    "                consecutive_rise = 0\n",
    "    \n",
    "            prev_perf = perf\n",
    "\n",
    "            # Progress\n",
    "            if (np.fmod(epoch,self.show) == 0 or len(stop) != 0):\n",
    "                print(this,end = \": \")\n",
    "                if np.isfinite(self.maxEpochs):\n",
    "                    print(\"Epoch \",epoch, \"/\", self.maxEpochs,end = \" \")\n",
    "                if np.isfinite(self.goal):\n",
    "                    print(\", Performance %8.3e\" % perf, \"/\", self.goal, end = \" \")\n",
    "                if np.isfinite(self.mingrad):\n",
    "                    print(\", Gradient %8.3e\" % normgX, \"/\", self.mingrad)\n",
    "\n",
    "                \n",
    "                if len(stop) != 0:\n",
    "                    print(\"\\n\",this,\":\",stop,\"\\n\")\n",
    "                    break\n",
    "            epochs = np.append(epochs,epoch)\n",
    "            perfs = np.append(perfs,perf)\n",
    "            self.train(gX_flattened)\n",
    "            \n",
    "        return perfs, epochs\n",
    "    def train(self,gX):\n",
    "        raise NotImplementedError(\"No se ha definido el optimizador, esta es la clase base\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class RmsProp(Optimizer):\\n    def __init__(self, nn: NeuralNetwork, inputs, targets,lr: float =1e-3, num_batch: int =1, maxEpochs: int =500, \\n                 goal: float =1e-8,mingrad: float =1e-11, show:int =1, error_fun=ErrorFunctions.SSE, \\n                 consecutive_epochs: int=10,WDecay:float=0,alpha:float=0.99,centered:bool=False,\\n                 momentum:float=0.6) -> None:\\n        super().__init__(lr,maxEpochs,goal,mingrad,nn,inputs,targets,error_fun,show,consecutive_epochs,num_batch)\\n        self.name = \"trainRMSPROP\"\\n        #Initial data\\n        self.v = 0\\n        self.vh = 0\\n        self.b = 0\\n        self.gAvg = 0\\n        self.WDecay = WDecay\\n        self.alpha = alpha\\n        self.centered = centered\\n        self.momentum = momentum\\n        \\n    def train(self,gX):\\n        # RMSProp\\n        if self.WDecay != 0:\\n            gX = gX + gX*self.WDecay\\n        self.v = self.alpha*self.v + ((1-self.alpha)*(gX**2))\\n        self.vh = self.v\\n        if self.centered:\\n            self.gAvg = self.gAvg*self.alpha + ((1-self.alpha)*gX)\\n            self.vh = self.vh - self.gAvg**2\\n        if self.momentum > 0:\\n            self.b = self.momentum*self.b + gX/((self.vh**(1/2))+1e-8)\\n            dX = -self.lr*self.b\\n            self.nn.weights += dX\\n        else:\\n            dX = -self.lr*(gX/((self.vh**(1/2))+1e-8))\\n            self.nn.weights += dX\\n        return 0'"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RmsProp(Optimizer):\n",
    "    def __init__(self, nn: NeuralNetwork, inputs, targets,lr: float =1e-3, num_batch: int =1, maxEpochs: int =500, \n",
    "                 goal: float =1e-8,mingrad: float =1e-11, show:int =1, error_fun=ErrorFunctions.SSE, \n",
    "                 consecutive_epochs: int=10,WDecay:float=0,alpha:float=0.99,centered:bool=False,\n",
    "                 momentum:float=0.6,epsilon:float=1e-9) -> None:\n",
    "        super().__init__(lr,maxEpochs,goal,mingrad,nn,inputs,targets,error_fun,show,consecutive_epochs,num_batch)\n",
    "        self.name = \"trainRMSPROP\"\n",
    "        self.epsilon = epsilon\n",
    "        self.v = np.zeros_like(np.concatenate([w.flatten() for w in nn.weights]))  # Vector de acumulación de gradientes\n",
    "        self.vh = 0\n",
    "        self.b = 0\n",
    "        self.gAvg = 0\n",
    "        self.WDecay = WDecay\n",
    "        self.alpha = alpha\n",
    "        self.centered = centered\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def train(self, gX):        \n",
    "        if self.WDecay != 0:\n",
    "            gX = gX + gX*self.WDecay\n",
    "        self.v = self.alpha*self.v + ((1-self.alpha)*(gX**2))\n",
    "        self.vh = self.v\n",
    "        if self.centered:\n",
    "            self.gAvg = self.gAvg*self.alpha + ((1-self.alpha)*gX)\n",
    "            self.vh = self.vh - self.gAvg**2\n",
    "        if self.momentum > 0:\n",
    "            self.b = self.momentum*self.b + gX/((self.vh**(1/2))+self.epsilon)\n",
    "            update = self.lr*self.b\n",
    "            #self.nn.weights += dX\n",
    "        else:\n",
    "            update = self.lr*(gX/((self.vh**(1/2))+1e-8))\n",
    "            #self.nn.weights += dX\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        start = 0\n",
    "        for i, w in enumerate(self.nn.weights):\n",
    "            shape = w.shape\n",
    "            size = np.prod(shape)\n",
    "            grad_update = update[start:start+size].reshape(shape)\n",
    "            self.nn.weights[i] -= grad_update\n",
    "            start += size\n",
    "\n",
    "\"\"\"class RmsProp(Optimizer):\n",
    "    def __init__(self, nn: NeuralNetwork, inputs, targets,lr: float =1e-3, num_batch: int =1, maxEpochs: int =500, \n",
    "                 goal: float =1e-8,mingrad: float =1e-11, show:int =1, error_fun=ErrorFunctions.SSE, \n",
    "                 consecutive_epochs: int=10,WDecay:float=0,alpha:float=0.99,centered:bool=False,\n",
    "                 momentum:float=0.6) -> None:\n",
    "        super().__init__(lr,maxEpochs,goal,mingrad,nn,inputs,targets,error_fun,show,consecutive_epochs,num_batch)\n",
    "        self.name = \"trainRMSPROP\"\n",
    "        #Initial data\n",
    "        self.v = 0\n",
    "        self.vh = 0\n",
    "        self.b = 0\n",
    "        self.gAvg = 0\n",
    "        self.WDecay = WDecay\n",
    "        self.alpha = alpha\n",
    "        self.centered = centered\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def train(self,gX):\n",
    "        # RMSProp\n",
    "        if self.WDecay != 0:\n",
    "            gX = gX + gX*self.WDecay\n",
    "        self.v = self.alpha*self.v + ((1-self.alpha)*(gX**2))\n",
    "        self.vh = self.v\n",
    "        if self.centered:\n",
    "            self.gAvg = self.gAvg*self.alpha + ((1-self.alpha)*gX)\n",
    "            self.vh = self.vh - self.gAvg**2\n",
    "        if self.momentum > 0:\n",
    "            self.b = self.momentum*self.b + gX/((self.vh**(1/2))+1e-8)\n",
    "            dX = -self.lr*self.b\n",
    "            self.nn.weights += dX\n",
    "        else:\n",
    "            dX = -self.lr*(gX/((self.vh**(1/2))+1e-8))\n",
    "            self.nn.weights += dX\n",
    "        return 0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "def main():\n",
    "    activationFunction()\n",
    "    neural_network = NeuralNetwork(input_size = 2,\n",
    "                                layer_sizes = [20, 40,20],\n",
    "                                output_size = 2,\n",
    "                                activation_funcs = [LeakyRelu(),LeakyRelu(),LeakyRelu(),Purelin()],\n",
    "                                wInit = 'random')\n",
    "\n",
    "    # Carga el archivo .mat\n",
    "    data = loadmat('engine_dataset.mat')\n",
    "    inputs = data['engineInputs'].T\n",
    "    targets = data['engineTargets'].T\n",
    "    for weight in neural_network.weights:\n",
    "        print(f\"Tamano de los pesos: {weight.shape}\")\n",
    "    \n",
    "    Optimizador = RmsProp(nn=neural_network,\n",
    "                          inputs=inputs,\n",
    "                          targets=targets,\n",
    "                          lr=1e-3,\n",
    "                          maxEpochs=10000,\n",
    "                          show=500,\n",
    "                          consecutive_epochs=10,\n",
    "                          mingrad=1e-8,\n",
    "                          error_fun=ErrorFunctions.MSE)\n",
    "    \n",
    "    perfs,epochs = Optimizador.optimize()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, perfs)\n",
    "    plt.title('Performance')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #print(f\"Pesos despues: {neural_network.weights}\")\n",
    "    outputs = neural_network.forwardPass(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamano de los pesos: (3, 20)\n",
      "Tamano de los pesos: (21, 40)\n",
      "Tamano de los pesos: (41, 20)\n",
      "Tamano de los pesos: (21, 2)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainRMSPROP: Epoch  0 / 10000 , Performance 3.486e+08 / 1e-08 , Gradient 7.810e+12 / 1e-08\n",
      "trainRMSPROP: Epoch  199 / 10000 , Performance 8.167e+05 / 1e-08 , Gradient 4.361e+11 / 1e-08\n",
      "\n",
      " trainRMSPROP : Performance has risen for 10 consecutive epochs \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABJn0lEQVR4nO3deXhU5d3/8c9MMpkkQkCEEJYAYZFFdhCMPhUXFiNVqBUt+hRcwKXwVI1Li7/KZjW2VtGn9QGtStSWqlDBVhGMaEDKjkQBlYIisZAEUZMAgWSSOb8/wgyMCZkAOXPuJO/XdeVq5syZme9855jOh/s+93FZlmUJAAAAAHBSbqcLAAAAAADTEZwAAAAAIAyCEwAAAACEQXACAAAAgDAITgAAAAAQBsEJAAAAAMIgOAEAAABAGAQnAAAAAAiD4AQAAAAAYRCcAADGevzxx9W5c2dFRUWpf//+TpcDAGjECE4AgFOSmZkpl8sV/ImNjdW5556rqVOnqqCgoM5e591339UDDzygiy66SPPnz9ejjz5aZ88NAMCpina6AABA/TR79mylpKTo6NGjWr16tebOnaulS5dq27Ztio+PP+Pnf//99+V2u/XCCy8oJiamDioGAOD0EZwAAKclLS1NgwcPliRNmjRJ55xzjp588km9+eabGj9+/Gk/b0lJieLj47V//37FxcXVWWiyLEtHjx5VXFxcnTwfAKBxYaoeAKBOXHbZZZKk3bt3S5L+8pe/aNCgQYqLi1OLFi30s5/9TF9//XXIYy655BL17t1bmzdv1sUXX6z4+Hg9+OCDcrlcmj9/vg4fPhycEpiZmSlJKi8v18MPP6wuXbrI6/WqU6dOevDBB1VaWhry3J06ddKPf/xjLV++XIMHD1ZcXJyeffZZZWdny+Vy6fXXX9esWbPUrl07NW3aVNdee62KiopUWlqqu+++W4mJiWrSpIluvvnmKs89f/58XXbZZUpMTJTX61WvXr00d+7cKj0J1LB69WoNGTJEsbGx6ty5s15++eUq+xYWFuqee+5Rp06d5PV61b59e02YMEEHDhwI7lNaWqoZM2aoa9eu8nq9Sk5O1gMPPFClPgBA3WPECQBQJ7744gtJ0jnnnKNHHnlEDz30kK677jpNmjRJ33zzjf74xz/q4osv1pYtW9S8efPg47799lulpaXpZz/7mf77v/9brVu31uDBg/Xcc89pw4YNev755yVJF154oaTK0a2XXnpJ1157re69916tX79eGRkZ+uyzz7R48eKQmnbs2KHx48fr9ttv1+TJk9W9e/fgfRkZGYqLi9Ovf/1r7dq1S3/84x/l8Xjkdrv1/fffa+bMmVq3bp0yMzOVkpKi6dOnBx87d+5cnXfeebr66qsVHR2tf/7zn/rFL34hv9+vKVOmhNSwa9cuXXvttbr11ls1ceJEvfjii7rppps0aNAgnXfeeZKkQ4cO6Uc/+pE+++wz3XLLLRo4cKAOHDigf/zjH/rPf/6jli1byu/36+qrr9bq1at12223qWfPntq6davmzJmjf//731qyZEmdfZYAgGpYAACcgvnz51uSrPfee8/65ptvrK+//tp69dVXrXPOOceKi4uzvvrqKysqKsp65JFHQh63detWKzo6OmT7sGHDLEnWvHnzqrzOxIkTrbPOOitkW05OjiXJmjRpUsj2++67z5Jkvf/++8FtHTt2tCRZy5YtC9n3gw8+sCRZvXv3tsrKyoLbx48fb7lcListLS1k/9TUVKtjx44h20pKSqrUO2rUKKtz584h2wI1rFq1Krht//79ltfrte69997gtunTp1uSrDfeeKPK8/r9fsuyLOuVV16x3G639eGHH4bcP2/ePEuS9a9//avKYwEAdYepegCA0zJ8+HC1atVKycnJ+tnPfqYmTZpo8eLFeuONN+T3+3XdddfpwIEDwZ+kpCR169ZNH3zwQcjzeL1e3XzzzbV6zaVLl0qS0tPTQ7bfe++9kqS33347ZHtKSopGjRpV7XNNmDBBHo8neHvo0KGyLEu33HJLyH5Dhw7V119/rfLy8uC2E8+TKioq0oEDBzRs2DB9+eWXKioqCnl8r1699KMf/Sh4u1WrVurevbu+/PLL4La///3v6tevn37yk59UqdPlckmSFi5cqJ49e6pHjx4hfQ1MkfxhXwEAdatRB6dVq1bpqquuUtu2beVyuU5rmsPy5ct1wQUXqGnTpmrVqpV++tOf6quvvqrzWgHANM8884yysrL0wQcf6NNPP9WXX36pUaNGaefOnbIsS926dVOrVq1Cfj777DPt378/5HnatWtX6wUg9uzZI7fbra5du4ZsT0pKUvPmzbVnz56Q7SkpKSd9rg4dOoTcbtasmSQpOTm5yna/3x8SiP71r39p+PDhOuuss9S8eXO1atVKDz74oCRVCU4/fB1JOvvss/X9998Hb3/xxRfq3bv3SWuVpJ07d2r79u1VenruuedKUpW+AgDqVqM+x+nw4cPq16+fbrnlFl1zzTWn/Pjdu3drzJgxSk9P11//+lcVFRXpnnvu0TXXXKOPPvrIhooBwBxDhgwJrqp3Ir/fL5fLpXfeeUdRUVFV7m/SpEnI7dNZ5S4wChNOTc9dXW01bbcsS1JlyLn88svVo0cPPfnkk0pOTlZMTIyWLl2qOXPmyO/3n9Lz1Zbf71efPn305JNPVnv/DwMfAKBuNerglJaWprS0tJPeX1paqv/3//6f/va3v6mwsFC9e/fW7373O11yySWSpM2bN6uiokK//e1v5XZXDt7dd999GjNmjHw+X8gUEABoLLp06SLLspSSkhIcDakrHTt2lN/v186dO9WzZ8/g9oKCAhUWFqpjx451+nrV+ec//6nS0lL94x//CBlNOpOpcl26dNG2bdvC7vPxxx/r8ssvr3VwBADUnUY9VS+cqVOnau3atXr11Vf1ySefaNy4cbriiiu0c+dOSdKgQYPkdrs1f/58VVRUqKioSK+88oqGDx9OaALQaF1zzTWKiorSrFmzqoyqWJalb7/99rSf+8orr5QkPfXUUyHbA6Mwo0ePPu3nrq3ACNKJ762oqEjz588/7ef86U9/qo8//rjKqoAnvs51112nvXv36s9//nOVfY4cOaLDhw+f9usDAMJr1CNONcnNzdX8+fOVm5urtm3bSqocTVq2bJnmz5+vRx99VCkpKXr33Xd13XXX6fbbb1dFRYVSU1ODJy8DQGPUpUsX/fa3v9W0adP01VdfaezYsWratKl2796txYsX67bbbtN99913Ws/dr18/TZw4Uc8995wKCws1bNgwbdiwQS+99JLGjh2rSy+9tI7fTVUjR45UTEyMrrrqKt1+++06dOiQ/vznPysxMVF5eXmn9Zz333+/Fi1apHHjxumWW27RoEGD9N133+kf//iH5s2bp379+unnP/+5Xn/9dd1xxx364IMPdNFFF6miokKff/65Xn/99eD1qgAA9iA4ncTWrVtVUVFRZZpJaWmpzjnnHElSfn6+Jk+erIkTJ2r8+PE6ePCgpk+frmuvvVZZWVlMpQDQaP3617/Wueeeqzlz5mjWrFmSKs/BGTlypK6++uozeu7nn39enTt3VmZmphYvXqykpCRNmzZNM2bMqIvSw+revbsWLVqk3/zmN7rvvvuUlJSkO++8U61ataqyIl9tNWnSRB9++KFmzJihxYsX66WXXlJiYqIuv/xytW/fXpLkdru1ZMkSzZkzRy+//LIWL16s+Ph4de7cWXfddVedT4sEAIRyWad6dmoD5XK5tHjxYo0dO1aS9Nprr+nGG2/U9u3bq5zY26RJEyUlJemhhx7SsmXLtHHjxuB9//nPf5ScnKy1a9fqggsuiORbAAAAAGATRpxOYsCAAaqoqND+/ftDrr9xopKSkuCiEAGBkPXDVZUAAAAA1F+NenGIQ4cOKScnRzk5OZIqlxfPyclRbm6uzj33XN14442aMGGC3njjDe3evVsbNmxQRkZG8AKLo0eP1saNGzV79mzt3LlTH330kW6++WZ17NhRAwYMcPCdAQAAAKhLjXqqXnZ2drUnEk+cOFGZmZny+Xz67W9/q5dffll79+5Vy5YtdcEFF2jWrFnq06ePJOnVV1/V73//e/373/9WfHy8UlNT9bvf/U49evSI9NsBAAAAYJNGHZwAAAAAoDYa9VQ9AAAAAKgNghMAAAAAhNHoVtXz+/3at2+fmjZtynWWAAAAgEbMsiwdPHhQbdu2rbJa9g81uuC0b98+JScnO10GAAAAAEN8/fXXwQuOn0yjC05NmzaVVNmchIQEh6uRfD6f3n33XY0cOVIej8fpchoc+ms/emw/emw/emw/emw/emw/emy/SPe4uLhYycnJwYxQk0YXnALT8xISEowJTvHx8UpISOA/QBvQX/vRY/vRY/vRY/vRY/vRY/vRY/s51ePanMLD4hAAAAAAEAbBCQAAAADCIDgBAAAAQBgEJwAAAAAIg+AEAAAAAGEQnAAAAAAgDIITAAAAAIRBcAIAAACAMAhOAAAAABCGo8Fp7ty56tu3rxISEpSQkKDU1FS98847J90/MzNTLpcr5Cc2NjaCFQMAAABojKKdfPH27dvrscceU7du3WRZll566SWNGTNGW7Zs0XnnnVftYxISErRjx47gbZfLFalyAQAAADRSjganq666KuT2I488orlz52rdunUnDU4ul0tJSUmRKA8AAAAAJDkcnE5UUVGhhQsX6vDhw0pNTT3pfocOHVLHjh3l9/s1cOBAPfrooycNWZJUWlqq0tLS4O3i4mJJks/nk8/nq7s3cJoCNZhQS0NEf+1Hj+1Hj+1Hj+1Hj+1Hj+1Hj+0X6R6fyuu4LMuybKwlrK1btyo1NVVHjx5VkyZNtGDBAl155ZXV7rt27Vrt3LlTffv2VVFRkf7whz9o1apV2r59u9q3b1/tY2bOnKlZs2ZV2b5gwQLFx8fX6Xs5VR/sc2n9freGJPp1WVtHPwYAAACg0SkpKdENN9ygoqIiJSQk1Liv48GprKxMubm5Kioq0qJFi/T8889r5cqV6tWrV9jH+nw+9ezZU+PHj9fDDz9c7T7VjTglJyfrwIEDYZtjt98v/7f+vPorXdLGr/+bfLk8Ho+j9TREPp9PWVlZGjFiBP21CT22Hz22Hz22Hz22Hz22Hz22X6R7XFxcrJYtW9YqODk+VS8mJkZdu3aVJA0aNEgbN27U008/rWeffTbsYz0ejwYMGKBdu3addB+v1yuv11vtY50+4L2eyvb7LTPqacjor/3osf3osf3osf3osf3osf3osf0i1eNTeQ3jruPk9/tDRohqUlFRoa1bt6pNmzY2V2WP6KjKFQErmKUHAAAAGM3REadp06YpLS1NHTp00MGDB7VgwQJlZ2dr+fLlkqQJEyaoXbt2ysjIkCTNnj1bF1xwgbp27arCwkI9/vjj2rNnjyZNmuTk2zhtnqjK3EpwAgAAAMzmaHDav3+/JkyYoLy8PDVr1kx9+/bV8uXLNWLECElSbm6u3O7jg2Lff/+9Jk+erPz8fJ199tkaNGiQ1qxZU6vzoUwU7WbECQAAAKgPHA1OL7zwQo33Z2dnh9yeM2eO5syZY2NFkRUdGHHyO1wIAAAAgBoZd45TY+I5do6TnxEnAAAAwGgEJwdFuznHCQAAAKgPCE4O4hwnAAAAoH4gODkomql6AAAAQL1AcHJQNMuRAwAAAPUCwclBnuBUPZfDlQAAAACoCcHJQYw4AQAAAPUDwclBgXOcCE4AAACA2QhODvKwHDkAAABQLxCcHMSqegAAAED9QHBykCcwVc/vcCEAAAAAakRwclAUU/UAAACAeoHg5KBoN1P1AAAAgPqA4OQgD8uRAwAAAPUCwclBLEcOAAAA1A8EJwexHDkAAABQPxCcHMSIEwAAAFA/EJwcxHWcAAAAgPqB4OSgwFQ9Sy5VkJ4AAAAAYxGcHBQYcZKkcq6CCwAAABiL4OSgaPfx9pcz4gQAAAAYi+DkoJARJ4ITAAAAYCyCk4Oi3UzVAwAAAOoDgpODXC5XMDz5GHECAAAAjEVwclhgul45F3MCAAAAjEVwclhggYhyP1P1AAAAAFMRnBzmOTbi5GPECQAAADAWwclhgXOcmKoHAAAAmIvg5LCoQHBiqh4AAABgLIKTw6KjAuc4MeIEAAAAmIrg5DAPU/UAAAAA4xGcHBZcjpypegAAAICxCE4OCy5HzogTAAAAYCyCk8OCy5FzjhMAAABgLIKTw4KLQ1QwVQ8AAAAwFcHJYVzHCQAAADAfwclh0UzVAwAAAIxHcHLY8REnpuoBAAAApiI4OSywql4FI04AAACAsRwNTnPnzlXfvn2VkJCghIQEpaam6p133qnxMQsXLlSPHj0UGxurPn36aOnSpRGq1h5M1QMAAADM52hwat++vR577DFt3rxZmzZt0mWXXaYxY8Zo+/bt1e6/Zs0ajR8/Xrfeequ2bNmisWPHauzYsdq2bVuEK687Hjer6gEAAACmczQ4XXXVVbryyivVrVs3nXvuuXrkkUfUpEkTrVu3rtr9n376aV1xxRW6//771bNnTz388MMaOHCg/vSnP0W48roTGHEqZ8QJAAAAMFa00wUEVFRUaOHChTp8+LBSU1Or3Wft2rVKT08P2TZq1CgtWbLkpM9bWlqq0tLS4O3i4mJJks/nk8/nO/PCz5DbVRmYSsvKjainoQn0lN7ahx7bjx7bjx7bjx7bjx7bjx7bL9I9PpXXcTw4bd26VampqTp69KiaNGmixYsXq1evXtXum5+fr9atW4dsa926tfLz80/6/BkZGZo1a1aV7e+++67i4+PPrPg6ULDPLcmtHTt3aumRfztdToOVlZXldAkNHj22Hz22Hz22Hz22Hz22Hz22X6R6XFJSUut9HQ9O3bt3V05OjoqKirRo0SJNnDhRK1euPGl4OlXTpk0LGaUqLi5WcnKyRo4cqYSEhDp5jTOx/h/btWb/XnXs1FlXjuzudDkNjs/nU1ZWlkaMGCGPx+N0OQ0SPbYfPbYfPbYfPbYfPbYfPbZfpHscmI1WG44Hp5iYGHXt2lWSNGjQIG3cuFFPP/20nn322Sr7JiUlqaCgIGRbQUGBkpKSTvr8Xq9XXq+3ynaPx2PEAR8THSVJ8sttRD0NlSmfd0NGj+1Hj+1Hj+1Hj+1Hj+1Hj+0XqR6fymsYdx0nv98fck7SiVJTU7VixYqQbVlZWSc9J6o+iApcANfPqnoAAACAqRwdcZo2bZrS0tLUoUMHHTx4UAsWLFB2draWL18uSZowYYLatWunjIwMSdJdd92lYcOG6YknntDo0aP16quvatOmTXruueecfBtnxBN1bDlyVtUDAAAAjOVocNq/f78mTJigvLw8NWvWTH379tXy5cs1YsQISVJubq7c7uODYhdeeKEWLFig3/zmN3rwwQfVrVs3LVmyRL1793bqLZyx6MCIUwXBCQAAADCVo8HphRdeqPH+7OzsKtvGjRuncePG2VRR5EUHR5yYqgcAAACYyrhznBqbwIiTjxEnAAAAwFgEJ4d5opiqBwAAAJiO4OQwpuoBAAAA5iM4OYypegAAAID5CE4Oi+Y6TgAAAIDxCE4Oiz52jlMF13ECAAAAjEVwclj0setUsTgEAAAAYC6Ck8MCq+r5GHECAAAAjEVwctjxESfOcQIAAABMRXByWOAcp3JGnAAAAABjEZwcFghOLEcOAAAAmIvg5DAPU/UAAAAA4xGcHMZUPQAAAMB8BCeHRQUugMuIEwAAAGAsgpPDglP1GHECAAAAjEVwchhT9QAAAADzEZwcFh2cqkdwAgAAAExFcHKYJ6ryI/D5OccJAAAAMBXByWHBqXqMOAEAAADGIjg5LDhVj3OcAAAAAGMRnBwWfWyqXoXfkmURngAAAAATEZwcFhhxkiQf0/UAAAAAIxGcHHZicKpguh4AAABgJIKTwwJT9SRW1gMAAABMRXBymOeEESdW1gMAAADMRHBymNvtkkuVgam8ghEnAAAAwEQEJwMcu5STfJzjBAAAABiJ4GSAQHBixAkAAAAwE8HJAMERJ85xAgAAAIxEcDKA+9inUM6qegAAAICRCE4GiDr2v6yqBwAAAJiJ4GSAqOCIE8EJAAAAMBHByQBuFocAAAAAjEZwMgCLQwAAAABmIzgZILgcOYtDAAAAAEYiOBng+HWcGHECAAAATERwMsDxqXqMOAEAAAAmIjgZ4PhUPUacAAAAABMRnAzgdlUGJkacAAAAADM5GpwyMjJ0/vnnq2nTpkpMTNTYsWO1Y8eOGh+TmZkpl8sV8hMbGxuhiu3BOU4AAACA2RwNTitXrtSUKVO0bt06ZWVlyefzaeTIkTp8+HCNj0tISFBeXl7wZ8+ePRGq2B6B6zhVMFUPAAAAMFK0ky++bNmykNuZmZlKTEzU5s2bdfHFF5/0cS6XS0lJSXaXFzHBxSFYjhwAAAAwkqPB6YeKiookSS1atKhxv0OHDqljx47y+/0aOHCgHn30UZ133nnV7ltaWqrS0tLg7eLiYkmSz+eTz+ero8pPn8/nU9Sxcb/SsnIjampIAv2kr/ahx/ajx/ajx/ajx/ajx/ajx/aLdI9P5XVclmUZMT/M7/fr6quvVmFhoVavXn3S/dauXaudO3eqb9++Kioq0h/+8AetWrVK27dvV/v27avsP3PmTM2aNavK9gULFig+Pr5O38PpenmnW5sPuDW2Y4UubWvExwEAAAA0eCUlJbrhhhtUVFSkhISEGvc1Jjjdeeedeuedd7R69epqA9DJ+Hw+9ezZU+PHj9fDDz9c5f7qRpySk5N14MCBsM2JBJ/Pp5vmrtCGb9y6f2Q33fajFKdLalB8Pp+ysrI0YsQIeTwep8tpkOix/eix/eix/eix/eix/eix/SLd4+LiYrVs2bJWwcmIqXpTp07VW2+9pVWrVp1SaJIkj8ejAQMGaNeuXdXe7/V65fV6q32cKQd84BwnSy5jampoTPq8Gyp6bD96bD96bD96bD96bD96bL9I9fhUXsPRVfUsy9LUqVO1ePFivf/++0pJOfXRloqKCm3dulVt2rSxocLICKyq52M5cgAAAMBIjo44TZkyRQsWLNCbb76ppk2bKj8/X5LUrFkzxcXFSZImTJigdu3aKSMjQ5I0e/ZsXXDBBeratasKCwv1+OOPa8+ePZo0aZJj7+NMBa/jxKp6AAAAgJEcDU5z586VJF1yySUh2+fPn6+bbrpJkpSbmyu3+/jA2Pfff6/JkycrPz9fZ599tgYNGqQ1a9aoV69ekSq7zrmDwYkRJwAAAMBEjgan2qxLkZ2dHXJ7zpw5mjNnjk0VOSM44sRUPQAAAMBIjp7jhErHgxNT9QAAAAATEZwMEOWqHGnyMVUPAAAAMBLByQBRxz4FRpwAAAAAMxGcDMA5TgAAAIDZCE4GCF7Hial6AAAAgJEITgZgcQgAAADAbAQnAwSCk4+pegAAAICRCE4GCEzVq/Az4gQAAACYiOBkgOBUPc5xAgAAAIxEcDLA8al6jDgBAAAAJiI4GYDlyAEAAACzEZwMEMVy5AAAAIDRCE4GYDlyAAAAwGwEJwO4j30KTNUDAAAAzERwMkCUqzIw+ViOHAAAADASwckALA4BAAAAmI3gZIDjF8AlOAEAAAAmIjgZgOs4AQAAAGYjOBkgOFWPEScAAADASAQnAzDiBAAAAJiN4GQAFocAAAAAzEZwMsDxqXqMOAEAAAAmIjgZwB2cqmfJshh1AgAAAExDcDJAYMRJYklyAAAAwEQEJwOcGJxYWQ8AAAAwD8HJAG6CEwAAAGA0gpMBok74FMpZkhwAAAAwDsHJACd+CD6WJAcAAACMQ3AygMsleY6d6MSS5AAAAIB5CE6GiD52ohMXwQUAAADMQ3AyRPSxE518nOMEAAAAGIfgZIjgiBOr6gEAAADGITgZIhCcGHECAAAAzENwMkRgql4FI04AAACAcQhOhjg+4kRwAgAAAExDcDJEcDlypuoBAAAAxiE4GSLaXflRsDgEAAAAYB6CkyGio1gcAgAAADAVwckQ0VFcABcAAAAwFcHJEJ7gVD1GnAAAAADTOBqcMjIydP7556tp06ZKTEzU2LFjtWPHjrCPW7hwoXr06KHY2Fj16dNHS5cujUC19opiVT0AAADAWI4Gp5UrV2rKlClat26dsrKy5PP5NHLkSB0+fPikj1mzZo3Gjx+vW2+9VVu2bNHYsWM1duxYbdu2LYKV173AVD2u4wQAAACYJ9rJF1+2bFnI7czMTCUmJmrz5s26+OKLq33M008/rSuuuEL333+/JOnhhx9WVlaW/vSnP2nevHlV9i8tLVVpaWnwdnFxsSTJ5/PJ5/PV1Vs5bYEajuUmHS0zo66GItBLemofemw/emw/emw/emw/emw/emy/SPf4VF7HZVmWMUMcu3btUrdu3bR161b17t272n06dOig9PR03X333cFtM2bM0JIlS/Txxx9X2X/mzJmaNWtWle0LFixQfHx8ndV+pv78uVvbvnfr+s4VurC1MR8JAAAA0GCVlJTohhtuUFFRkRISEmrc19ERpxP5/X7dfffduuiii04amiQpPz9frVu3DtnWunVr5efnV7v/tGnTlJ6eHrxdXFys5ORkjRw5MmxzIsHn8ykrK0ttk1pr2/ffqGev83Tl0A5Ol9VgBPo7YsQIeTwep8tpkOix/eix/eix/eix/eix/eix/SLd48BstNowJjhNmTJF27Zt0+rVq+v0eb1er7xeb5XtHo/HqAPeEx0lSfLLbVRdDYVpn3dDRI/tR4/tR4/tR4/tR4/tR4/tF6ken8prGBGcpk6dqrfeekurVq1S+/bta9w3KSlJBQUFIdsKCgqUlJRkZ4m28xxbVY/lyAEAAADzOLqqnmVZmjp1qhYvXqz3339fKSkpYR+TmpqqFStWhGzLyspSamqqXWVGRHRU5UfBcuQAAACAeRwdcZoyZYoWLFigN998U02bNg2ep9SsWTPFxcVJkiZMmKB27dopIyNDknTXXXdp2LBheuKJJzR69Gi9+uqr2rRpk5577jnH3kddCCxHXk5wAgAAAIzj6IjT3LlzVVRUpEsuuURt2rQJ/rz22mvBfXJzc5WXlxe8feGFF2rBggV67rnn1K9fPy1atEhLliypcUGJ+iCaqXoAAACAsRwdcarNSujZ2dlVto0bN07jxo2zoSLnHA9OjDgBAAAApnF0xAnHBc5xKq9gxAkAAAAwDcHJEIFV9VgcAgAAADAPwckQwcUhOMcJAAAAMA7ByRDR7sBUPUacAAAAANMQnAwRGHFiqh4AAABgHoKTITyBxSGYqgcAAAAYh+BkiOBy5Iw4AQAAAMYhOBkiKriqHiNOAAAAgGkITobwHDvHqYIL4AIAAADGITgZIrCqno/gBAAAABiH4GSI4HWcmKoHAAAAGIfgZAgWhwAAAADMRXAyRGA5ch/LkQMAAADGITgZ4vhUPUacAAAAANMQnAwRXByCc5wAAAAA4xCcDBE8x4lV9QAAAADjnHJwmjhxolatWmVHLY1aNNdxAgAAAIx1ysGpqKhIw4cPV7du3fToo49q7969dtTV6ARGnJiqBwAAAJjnlIPTkiVLtHfvXt1555167bXX1KlTJ6WlpWnRokXy+Xx21NgoBFbVY3EIAAAAwDyndY5Tq1atlJ6ero8//ljr169X165d9fOf/1xt27bVPffco507d9Z1nQ3e8XOcGHECAAAATHNGi0Pk5eUpKytLWVlZioqK0pVXXqmtW7eqV69emjNnTl3V2ChEB67jxIgTAAAAYJxTDk4+n09///vf9eMf/1gdO3bUwoULdffdd2vfvn166aWX9N577+n111/X7Nmz7ai3wTp+HSdGnAAAAADTRJ/qA9q0aSO/36/x48drw4YN6t+/f5V9Lr30UjVv3rwOyms8PIHFIVhVDwAAADDOKQenOXPmaNy4cYqNjT3pPs2bN9fu3bvPqLDGJjq4OAQjTgAAAIBpTjk4/fznP7ejjkYv6tiIk9+S/H5L7mO3AQAAADjvjBaHQN3xnBCUypmuBwAAABiF4GSIwOIQEkuSAwAAAKYhOBki2n38o2BJcgAAAMAsBCdDeE4ccWKBCAAAAMAoBCdDuFyu4AIRnOMEAAAAmIXgZJCYY0uSl5Uz4gQAAACYhOBkkJjoyo+jlOAEAAAAGIXgZBBvMDhVOFwJAAAAgBMRnAwSGHFiqh4AAABgFoKTQbxM1QMAAACMRHAySEx0lCRGnAAAAADTEJwMwlQ9AAAAwEwEJ4MwVQ8AAAAwE8HJIIHgVFbBqnoAAACASRwNTqtWrdJVV12ltm3byuVyacmSJTXun52dLZfLVeUnPz8/MgXbLDji5GPECQAAADCJo8Hp8OHD6tevn5555plTetyOHTuUl5cX/ElMTLSpwsgKnuNUQXACAAAATBLt5IunpaUpLS3tlB+XmJio5s2b131BDvMeW1WPEScAAADALI4Gp9PVv39/lZaWqnfv3po5c6Yuuuiik+5bWlqq0tLS4O3i4mJJks/nk8/ns73WcAI1+Hw+HRtw0pEyM2prCE7sL+xBj+1Hj+1Hj+1Hj+1Hj+1Hj+0X6R6fyuu4LMuybKyl1lwulxYvXqyxY8eedJ8dO3YoOztbgwcPVmlpqZ5//nm98sorWr9+vQYOHFjtY2bOnKlZs2ZV2b5gwQLFx8fXVfl1YtGXbn1Y4Nao9n5dmcyoEwAAAGCnkpIS3XDDDSoqKlJCQkKN+9ar4FSdYcOGqUOHDnrllVeqvb+6Eafk5GQdOHAgbHMiwefzKSsrSyNGjNAf3vtSL67Zo8n/1UkPjDrX6dIahBP76/F4nC6nQaLH9qPH9qPH9qPH9qPH9qPH9ot0j4uLi9WyZctaBad6OVXvREOGDNHq1atPer/X65XX662y3ePxGHXAezwexcZUfhzlloyqrSEw7fNuiOix/eix/eix/eix/eix/eix/SLV41N5jXp/HaecnBy1adPG6TLqRHBxCC6ACwAAABjF0RGnQ4cOadeuXcHbu3fvVk5Ojlq0aKEOHTpo2rRp2rt3r15++WVJ0lNPPaWUlBSdd955Onr0qJ5//nm9//77evfdd516C3UquBw5wQkAAAAwiqPBadOmTbr00kuDt9PT0yVJEydOVGZmpvLy8pSbmxu8v6ysTPfee6/27t2r+Ph49e3bV++9917Ic9RnwQvgEpwAAAAAozganC655BLVtDZFZmZmyO0HHnhADzzwgM1VOef4iFOFw5UAAAAAOFG9P8epIfEyVQ8AAAAwEsHJIDFM1QMAAACMRHAyCCNOAAAAgJkITgZhOXIAAADATAQng7AcOQAAAGAmgpNBji9Hzqp6AAAAgEkITgZhxAkAAAAwE8HJIJzjBAAAAJiJ4GQQRpwAAAAAMxGcDBI8x6mC4AQAAACYhOBkkBNHnCzLcrgaAAAAAAEEJ4MEgpMklTHqBAAAABiD4GQQ7wnBiQUiAAAAAHMQnAwSE3XCiBPBCQAAADAGwckgLpcrOF2PEScAAADAHAQnw3ijWJIcAAAAMA3ByTBeD8EJAAAAMA3ByTCB85xKyyscrgQAAABAAMHJMCdeywkAAACAGQhOhvFGR0licQgAAADAJAQnwzDiBAAAAJiH4GQYbzTnOAEAAACmITgZhus4AQAAAOYhOBnGy1Q9AAAAwDgEJ8Mw4gQAAACYh+BkmMCqeow4AQAAAOYgOBmGEScAAADAPAQnw7AcOQAAAGAegpNhWI4cAAAAMA/ByTCMOAEAAADmITgZJrA4BOc4AQAAAOYgOBmG6zgBAAAA5iE4GSYYnCoITgAAAIApCE6GiWFxCAAAAMA4BCfDxEQxVQ8AAAAwDcHJMF4PF8AFAAAATENwMkxMFKvqAQAAAKYhOBnm+AVwCU4AAACAKQhOhuECuAAAAIB5HA1Oq1at0lVXXaW2bdvK5XJpyZIlYR+TnZ2tgQMHyuv1qmvXrsrMzLS9zkg6fh0nVtUDAAAATOFocDp8+LD69eunZ555plb77969W6NHj9all16qnJwc3X333Zo0aZKWL19uc6WRE8NUPQAAAMA40U6+eFpamtLS0mq9/7x585SSkqInnnhCktSzZ0+tXr1ac+bM0ahRo+wqM6K80ZWLQzBVDwAAADCHo8HpVK1du1bDhw8P2TZq1CjdfffdJ31MaWmpSktLg7eLi4slST6fTz6fz5Y6T0WghsD/ulUZmErLK4yor777YX9R9+ix/eix/eix/eix/eix/eix/SLd41N5nXoVnPLz89W6deuQba1bt1ZxcbGOHDmiuLi4Ko/JyMjQrFmzqmx/9913FR8fb1utpyorK0uS9O1RSYrWkVKfli5d6mhNDUmgv7APPbYfPbYfPbYfPbYfPbYfPbZfpHpcUlJS633rVXA6HdOmTVN6enrwdnFxsZKTkzVy5EglJCQ4WFkln8+nrKwsjRgxQh6PR/sPlmr2lpUqt1y68sornS6v3vthf1H36LH96LH96LH96LH96LH96LH9It3jwGy02qhXwSkpKUkFBQUh2woKCpSQkFDtaJMkeb1eeb3eKts9Ho9RB3ygnrNiLUmS35Jc7ihFR7FifF0w7fNuiOix/eix/eix/eix/eix/eix/SLV41N5jXr1rTw1NVUrVqwI2ZaVlaXU1FSHKqp7gcUhJFbWAwAAAEzhaHA6dOiQcnJylJOTI6lyufGcnBzl5uZKqpxmN2HChOD+d9xxh7788ks98MAD+vzzz/V///d/ev3113XPPfc4Ub4tAsuRS6ysBwAAAJjC0eC0adMmDRgwQAMGDJAkpaena8CAAZo+fbokKS8vLxiiJCklJUVvv/22srKy1K9fPz3xxBN6/vnnG8xS5JIU5XYp2u2SJJVVEJwAAAAAEzh6jtMll1wiy7JOen9mZma1j9myZYuNVTkvJtqt8rIKlfoITgAAAIAJ6tU5To1FYLpeWUWFw5UAAAAAkAhORvIeC05HGXECAAAAjEBwMtDxESeCEwAAAGACgpOBAkuSc44TAAAAYAaCk4FiohhxAgAAAExCcDKQ13MsOHEdJwAAAMAIBCcDBUacSstZVQ8AAAAwAcHJQF5P5TlOjDgBAAAAZiA4Gej4iBPBCQAAADABwclAges4MeIEAAAAmIHgZKBAcOIcJwAAAMAMBCcDxTDiBAAAABiF4GQgpuoBAAAAZiE4GSgmmsUhAAAAAJMQnAzkja5cjpzgBAAAAJiB4GQgRpwAAAAAsxCcDMTiEAAAAIBZCE4GYjlyAAAAwCwEJwMx4gQAAACYheBkIBaHAAAAAMxCcDIQI04AAACAWQhOBgpeALeC4AQAAACYgOBkoBgWhwAAAACMQnAykJepegAAAIBRCE4G8nIBXAAAAMAoBCcDxURVrqrHiBMAAABgBoKTgbweRpwAAAAAkxCcDBQTxTlOAAAAgEkITgYKjDgRnAAAAAAzEJwMFBxxqvDL77ccrgYAAAAAwclAXk9U8HcuggsAAAA4j+BkoMCIk8QCEQAAAIAJCE4G8kS5gr9znhMAAADgPIKTgVwu1wkXwa1wuBoAAAAABCdDxUSzsh4AAABgCoKTobzRlQtEsDgEAAAA4DyCk6GCU/V8BCcAAADAaQQnQwWCEyNOAAAAgPMIToaKYcQJAAAAMIYRwemZZ55Rp06dFBsbq6FDh2rDhg0n3TczM1MulyvkJzY2NoLVRsbxESdW1QMAAACc5nhweu2115Senq4ZM2boo48+Ur9+/TRq1Cjt37//pI9JSEhQXl5e8GfPnj0RrDgyGHECAAAAzOF4cHryySc1efJk3XzzzerVq5fmzZun+Ph4vfjiiyd9jMvlUlJSUvCndevWEaw4MmI4xwkAAAAwRrSTL15WVqbNmzdr2rRpwW1ut1vDhw/X2rVrT/q4Q4cOqWPHjvL7/Ro4cKAeffRRnXfeedXuW1paqtLS0uDt4uJiSZLP55PP56ujd3L6AjX8sBaP2yVJOnzUjDrrq5P1F3WHHtuPHtuPHtuPHtuPHtuPHtsv0j0+lddxWZZl2VhLjfbt26d27dppzZo1Sk1NDW5/4IEHtHLlSq1fv77KY9auXaudO3eqb9++Kioq0h/+8AetWrVK27dvV/v27avsP3PmTM2aNavK9gULFig+Pr5u31AdemGHW59859a4lAr9V5JjHxEAAADQYJWUlOiGG25QUVGREhISatzX0RGn05GamhoSsi688EL17NlTzz77rB5++OEq+0+bNk3p6enB28XFxUpOTtbIkSPDNicSfD6fsrKyNGLECHk8nuD29w5/ok++y1e3Hr105YUdHaywfjtZf1F36LH96LH96LH96LH96LH96LH9It3jwGy02nA0OLVs2VJRUVEqKCgI2V5QUKCkpKRaPYfH49GAAQO0a9euau/3er3yer3VPs6kA/6H9cR6Kj+acktG1VlfmfZ5N0T02H702H702H702H702H702H6R6vGpvIaji0PExMRo0KBBWrFiRXCb3+/XihUrQkaValJRUaGtW7eqTZs2dpXpCK/n2OIQ5SwOAQAAADjN8al66enpmjhxogYPHqwhQ4boqaee0uHDh3XzzTdLkiZMmKB27dopIyNDkjR79mxdcMEF6tq1qwoLC/X4449rz549mjRpkpNvo87FREVJkkoJTgAAAIDjHA9O119/vb755htNnz5d+fn56t+/v5YtWxZcYjw3N1du9/GBse+//16TJ09Wfn6+zj77bA0aNEhr1qxRr169nHoLtmDECQAAADCH48FJkqZOnaqpU6dWe192dnbI7Tlz5mjOnDkRqMpZMVHHLoBbXuFwJQAAAAAcvwAuqhe8AC4jTgAAAIDjCE6G8hKcAAAAAGMQnAzVxFs5i7LoCFemBgAAAJxGcDJUUrNYSVJ+canDlQAAAAAgOBmqTbM4SVJ+0RGHKwEAAABAcDJUYMTp+xKfjvpYWQ8AAABwEsHJUAmx0YqPqbwIbn7RUYerAQAAABo3gpOhXC5XcNQpj+AEAAAAOIrgZLA2wQUiOM8JAAAAcBLByWBJCZULRDDiBAAAADiL4GSw4IgTwQkAAABwFMHJYJzjBAAAAJiB4GQwRpwAAAAAMxCcDMaIEwAAAGAGgpPB2jSrXBziwKFSlZX7Ha4GAAAAaLwITgY7O96jmOjKj6igmFEnAAAAwCkEJ4O5XK4TruVEcAIAAACcQnAyXFIC5zkBAAAATiM4Ge74ynpHHK4EAAAAaLwIToZLOrZABCNOAAAAgHMITobjWk4AAACA8whOhuNaTgAAAIDzCE6GY8QJAAAAcB7ByXCBEaf9B4+qvIKL4AIAAABOIDgZruVZXkW7XfJb0jeHSp0uBwAAAGiUCE6Gc7tdas21nAAAAABHEZzqAc5zAgAAAJxFcKoHwq2st/Gr7zTzH9u1I/9gJMsCAAAAGo1opwtAeMdHnI5Uuc+yLN238GPt+bZEr6zbo/8e2kH3jDhXzeNjIl0mAAAA0GAx4lQPJDWLk1T9iNOWrwu159sSuV1Shd/SS2v36JI/ZOvNnL2RLhMAAABosAhO9UBN5zgt2VIZkMb0b6e/Thqq7q2bqrDEp/TXP9ZHud9HtE4AAACgoSI41QMnW1XPV+HXW5/kSZLGDmini7q21Nu//C+N7ttGFX5Lv/zbFhUf9UW8XgAAAKChITjVA4ERp4Lio/L7reD21TsP6LvDZWrZJEYXdTlHkhQd5VbGNX3U/uw4/ef7I/p/i7fJsqxqnxcAAABA7RCc6oFWTb1yu6Ryv6UDh49fBHfxsWl6P+7bVtFRxz/KhFiP/nf8AEW5Xfrnx/u0aPN/Il4zAAAA0JAQnOoBT5RbiU0rR50Wf1QZlg6Xlivr0wJJldP0fmhgh7OVPuJcSdKMf2zXF98cilC1AAAAQMNDcKonbr6okyQp453P9fLar/Tup/k64qtQSsuz1K99s2ofc8ewLrqwyzkqKavQL/+2RaXlFRGsGAAAAGg4CE71xG0Xd9YvLukiSZr+5nb97p0dkqQx/dvK5XJV+5got0tzru+vs+M92r6vWL9ftiNi9QIAAAANCcGpnnC5XLp/VHfddnFnSVJ+ceUKe2P7V52md6LWCbF6/Np+kqQXVu/WBzv221soAAAA0AARnOoRl8ulaWk9dMtFKZKk8zudrU4tzwr7uOG9WuumCztJku57/WPtP1j1elAAAAAATs6I4PTMM8+oU6dOio2N1dChQ7Vhw4Ya91+4cKF69Oih2NhY9enTR0uXLo1Qpc5zuVx66Mc9tWDSUD1z48BaP+7XaT3UI6mpvj1cpttf2azt+4psrBIAAABoWBwPTq+99prS09M1Y8YMffTRR+rXr59GjRql/furn1K2Zs0ajR8/Xrfeequ2bNmisWPHauzYsdq2bVuEK3eOy+XShV1bBlfaq41YT5T+dMMAxcdEaUtuoUb/72rd9vImbdtLgAIAAADCiXa6gCeffFKTJ0/WzTffLEmaN2+e3n77bb344ov69a9/XWX/p59+WldccYXuv/9+SdLDDz+srKws/elPf9K8efMiWnt90zWxqf75P/+lp9/bqX9+sk/vflqgdz8tULM4j5JbxKlDi3idFROt70t8KiwpU9ERn/yWJZfLJbdLinK7FedxKy4mSnGeKMV6Kv835Pax3+M8UYo99rs32i33seeQS3Kp8neXyyWXS3Kp6u/uY/u5XDq2/fjvbpfr2H7HHqfj/+s+9jw6dr+/okKFpZXnhHk9FcHXr/Z5TvL6Jwo+t1w/uK3gIh2uH+57ksU7AAAAGgvLsnSotFzFR8tVfMSn4iM+De7UQlHu+vM9yWVZluXUi5eVlSk+Pl6LFi3S2LFjg9snTpyowsJCvfnmm1Ue06FDB6Wnp+vuu+8ObpsxY4aWLFmijz/+uMr+paWlKi09ftHY4uJiJScn68CBA0pISKjT93M6fD6fsrKyNGLECHk8noi97q79h/RM9pdaui1ffseOgMbpxLBVedtVJWzphPtC9z35Y1VDqAu9HXp/Tfed+DrB2z+4T5KOHj2q2Njaj4CeaZg8k0efyUufUdVn8MIuWSopOaL4+LhTruLM3u8Z1HyG/z8Y6c/YsqTDhw/rrLPOOuPaT+U1Iy3SL3nie7RkqaSkRPHx8Wd0bIV9zQi/S6M+R8vSkSNHFBcXd+b/Ef7geSPNyWO1xv1k6eiRo4qNiz2j49iJr16R/spvSarwWyor96uswq/Scn+VPm+cdqmax4d+/430d+Pi4mK1bNlSRUVFYbOBoyNOBw4cUEVFhVq3bh2yvXXr1vr888+rfUx+fn61++fn51e7f0ZGhmbNmlVl+7vvvqv4+PjTrLzuZWVlRfw1RzSRLj5f+rZU+u6oSwdKpbIK6SyPdFa0FB8tuV2WLKvyD0O5Jfn8lfv4/FLZsR9fhev47/4T7quQfH6XfH7Jr+N/lAL/zVhW5e/WsY3WCff5T9z3hPuq7Hta9zn7Lxs/7EPNf63rS6p1SWWl4XfDGXDp21IWdrGXSzpa4nQRDZxLOnrE6SIaOJfE3wqb8f95ZyLKZSk+WoqLkpYuz1Jzb/X7Req7cUlJ7f/uOz5Vz27Tpk1Tenp68HZgxGnkyJGNesSpsaipv5ZlyW9V/q+lY2Ht2O+WJfmPBZoTg55O2HJiADr+e9X7Ttzww+eydPxfgKq+zvE6q3+sVc3rnKSOKqHVCv+YGl7vROXl5Vq3bp0uuOACRUeH/5Nypv/gdSb/mnwmr30mZZ/Jv/JZkirKy7VhwwYNGTJEUbXocciDz+B1T/uxZ/ghn9lrn97jysvLtXHTRp0/+PxaHcd2cHJWbyReury8XBs3btT554f22MnpzI7+M5oNL17bvxVOvu/6/nmXl5dr/Yb1Gjpk6Cn9rXD2v2/nXjzK7ZI32q2YaLe80W4lxEbL64mq8TFOjDjVlqPBqWXLloqKilJBQUHI9oKCAiUlJVX7mKSkpFPa3+v1yuutGmU9Ho9RQcW0ehoa+msfn8+nvdukfh1a0GOb+Hw+7f9MGpzSkh7bxOfz6ft/S6ldW9Fjm/h8Pn27QxrahR7bhb8V9vP5fMr/VBrY6Rx6bLNIfXc7lddwdFW9mJgYDRo0SCtWrAhu8/v9WrFihVJTU6t9TGpqasj+UuVQ3sn2BwAAAIAz5fhUvfT0dE2cOFGDBw/WkCFD9NRTT+nw4cPBVfYmTJigdu3aKSMjQ5J01113adiwYXriiSc0evRovfrqq9q0aZOee+45J98GAAAAgAbM8eB0/fXX65tvvtH06dOVn5+v/v37a9myZcEFIHJzc+V2Hx8Yu/DCC7VgwQL95je/0YMPPqhu3bppyZIl6t27t1NvAQAAAEAD53hwkqSpU6dq6tSp1d6XnZ1dZdu4ceM0btw4m6sCAAAAgEqOnuMEAAAAAPUBwQkAAAAAwiA4AQAAAEAYBCcAAAAACIPgBAAAAABhEJwAAAAAIAyCEwAAAACEQXACAAAAgDAITgAAAAAQBsEJAAAAAMKIdrqASLMsS5JUXFzscCWVfD6fSkpKVFxcLI/H43Q5DQ79tR89th89th89th89th89th89tl+kexzIBIGMUJNGF5wOHjwoSUpOTna4EgAAAAAmOHjwoJo1a1bjPi6rNvGqAfH7/dq3b5+aNm0ql8vldDkqLi5WcnKyvv76ayUkJDhdToNDf+1Hj+1Hj+1Hj+1Hj+1Hj+1Hj+0X6R5blqWDBw+qbdu2crtrPoup0Y04ud1utW/f3ukyqkhISOA/QBvRX/vRY/vRY/vRY/vRY/vRY/vRY/tFssfhRpoCWBwCAAAAAMIgOAEAAABAGAQnh3m9Xs2YMUNer9fpUhok+ms/emw/emw/emw/emw/emw/emw/k3vc6BaHAAAAAIBTxYgTAAAAAIRBcAIAAACAMAhOAAAAABAGwQkAAAAAwiA4OeiZZ55Rp06dFBsbq6FDh2rDhg1Ol1RvZWRk6Pzzz1fTpk2VmJiosWPHaseOHSH7XHLJJXK5XCE/d9xxh0MV1z8zZ86s0r8ePXoE7z969KimTJmic845R02aNNFPf/pTFRQUOFhx/dOpU6cqPXa5XJoyZYokjuHTsWrVKl111VVq27atXC6XlixZEnK/ZVmaPn262rRpo7i4OA0fPlw7d+4M2ee7777TjTfeqISEBDVv3ly33nqrDh06FMF3Ybaaeuzz+fSrX/1Kffr00VlnnaW2bdtqwoQJ2rdvX8hzVHfsP/bYYxF+J+YKdxzfdNNNVfp3xRVXhOzDcVyzcD2u7m+zy+XS448/HtyH4/jkavM9rTbfI3JzczV69GjFx8crMTFR999/v8rLyyP2PghODnnttdeUnp6uGTNm6KOPPlK/fv00atQo7d+/3+nS6qWVK1dqypQpWrdunbKysuTz+TRy5EgdPnw4ZL/JkycrLy8v+PP73//eoYrrp/POOy+kf6tXrw7ed8899+if//ynFi5cqJUrV2rfvn265pprHKy2/tm4cWNIf7OysiRJ48aNC+7DMXxqDh8+rH79+umZZ56p9v7f//73+t///V/NmzdP69ev11lnnaVRo0bp6NGjwX1uvPFGbd++XVlZWXrrrbe0atUq3XbbbZF6C8arqcclJSX66KOP9NBDD+mjjz7SG2+8oR07dujqq6+usu/s2bNDju3/+Z//iUT59UK441iSrrjiipD+/e1vfwu5n+O4ZuF6fGJv8/Ly9OKLL8rlcumnP/1pyH4cx9Wrzfe0cN8jKioqNHr0aJWVlWnNmjV66aWXlJmZqenTp0fujVhwxJAhQ6wpU6YEb1dUVFht27a1MjIyHKyq4di/f78lyVq5cmVw27Bhw6y77rrLuaLquRkzZlj9+vWr9r7CwkLL4/FYCxcuDG777LPPLEnW2rVrI1Rhw3PXXXdZXbp0sfx+v2VZHMNnSpK1ePHi4G2/328lJSVZjz/+eHBbYWGh5fV6rb/97W+WZVnWp59+akmyNm7cGNznnXfesVwul7V3796I1V5f/LDH1dmwYYMlydqzZ09wW8eOHa05c+bYW1wDUV2PJ06caI0ZM+akj+E4PjW1OY7HjBljXXbZZSHbOI5r74ff02rzPWLp0qWW2+228vPzg/vMnTvXSkhIsEpLSyNSNyNODigrK9PmzZs1fPjw4Da3263hw4dr7dq1DlbWcBQVFUmSWrRoEbL9r3/9q1q2bKnevXtr2rRpKikpcaK8emvnzp1q27atOnfurBtvvFG5ubmSpM2bN8vn84Uc0z169FCHDh04pk9TWVmZ/vKXv+iWW26Ry+UKbucYrju7d+9Wfn5+yHHbrFkzDR06NHjcrl27Vs2bN9fgwYOD+wwfPlxut1vr16+PeM0NQVFRkVwul5o3bx6y/bHHHtM555yjAQMG6PHHH4/o9JuGIDs7W4mJierevbvuvPNOffvtt8H7OI7rVkFBgd5++23deuutVe7jOK6dH35Pq833iLVr16pPnz5q3bp1cJ9Ro0apuLhY27dvj0jd0RF5FYQ4cOCAKioqQj54SWrdurU+//xzh6pqOPx+v+6++25ddNFF6t27d3D7DTfcoI4dO6pt27b65JNP9Ktf/Uo7duzQG2+84WC19cfQoUOVmZmp7t27Ky8vT7NmzdKPfvQjbdu2Tfn5+YqJianyRah169bKz893puB6bsmSJSosLNRNN90U3MYxXLcCx2Z1f4sD9+Xn5ysxMTHk/ujoaLVo0YJj+zQcPXpUv/rVrzR+/HglJCQEt//yl7/UwIED1aJFC61Zs0bTpk1TXl6ennzySQerrT+uuOIKXXPNNUpJSdEXX3yhBx98UGlpaVq7dq2ioqI4juvYSy+9pKZNm1aZjs5xXDvVfU+rzfeI/Pz8av9eB+6LBIITGpwpU6Zo27ZtIeffSAqZy92nTx+1adNGl19+ub744gt16dIl0mXWO2lpacHf+/btq6FDh6pjx456/fXXFRcX52BlDdMLL7ygtLQ0tW3bNriNYxj1mc/n03XXXSfLsjR37tyQ+9LT04O/9+3bVzExMbr99tuVkZEhr9cb6VLrnZ/97GfB3/v06aO+ffuqS5cuys7O1uWXX+5gZQ3Tiy++qBtvvFGxsbEh2zmOa+dk39PqA6bqOaBly5aKioqqslJIQUGBkpKSHKqqYZg6dareeustffDBB2rfvn2N+w4dOlSStGvXrkiU1uA0b95c5557rnbt2qWkpCSVlZWpsLAwZB+O6dOzZ88evffee5o0aVKN+3EMn5nAsVnT3+KkpKQqi/aUl5fru+++49g+BYHQtGfPHmVlZYWMNlVn6NChKi8v11dffRWZAhuYzp07q2XLlsG/DRzHdefDDz/Ujh07wv59ljiOq3Oy72m1+R6RlJRU7d/rwH2RQHByQExMjAYNGqQVK1YEt/n9fq1YsUKpqakOVlZ/WZalqVOnavHixXr//feVkpIS9jE5OTmSpDZt2thcXcN06NAhffHFF2rTpo0GDRokj8cTckzv2LFDubm5HNOnYf78+UpMTNTo0aNr3I9j+MykpKQoKSkp5LgtLi7W+vXrg8dtamqqCgsLtXnz5uA+77//vvx+fzC4omaB0LRz50699957Ouecc8I+JicnR263u8r0MtTOf/7zH3377bfBvw0cx3XnhRde0KBBg9SvX7+w+3IcHxfue1ptvkekpqZq69atIf8IEPiHmF69ekXsjcABr776quX1eq3MzEzr008/tW677TarefPmISuFoPbuvPNOq1mzZlZ2draVl5cX/CkpKbEsy7J27dplzZ4929q0aZO1e/du680337Q6d+5sXXzxxQ5XXn/ce++9VnZ2trV7927rX//6lzV8+HCrZcuW1v79+y3Lsqw77rjD6tChg/X+++9bmzZtslJTU63U1FSHq65/KioqrA4dOli/+tWvQrZzDJ+egwcPWlu2bLG2bNliSbKefPJJa8uWLcEV3R577DGrefPm1ptvvml98skn1pgxY6yUlBTryJEjwee44oorrAEDBljr16+3Vq9ebXXr1s0aP368U2/JODX1uKyszLr66qut9u3bWzk5OSF/nwOrYK1Zs8aaM2eOlZOTY33xxRfWX/7yF6tVq1bWhAkTHH5n5qipxwcPHrTuu+8+a+3atdbu3but9957zxo4cKDVrVs36+jRo8Hn4DiuWbi/FZZlWUVFRVZ8fLw1d+7cKo/nOK5ZuO9plhX+e0R5ebnVu3dva+TIkVZOTo61bNkyq1WrVta0adMi9j4ITg764x//aHXo0MGKiYmxhgwZYq1bt87pkuotSdX+zJ8/37Isy8rNzbUuvvhiq0WLFpbX67W6du1q3X///VZRUZGzhdcj119/vdWmTRsrJibGateunXX99ddbu3btCt5/5MgR6xe/+IV19tlnW/Hx8dZPfvITKy8vz8GK66fly5dbkqwdO3aEbOcYPj0ffPBBtX8bJk6caFlW5ZLkDz30kNW6dWvL6/Val19+eZXef/vtt9b48eOtJk2aWAkJCdbNN99sHTx40IF3Y6aaerx79+6T/n3+4IMPLMuyrM2bN1tDhw61mjVrZsXGxlo9e/a0Hn300ZAv/Y1dTT0uKSmxRo4cabVq1cryeDxWx44drcmTJ1f5h1iO45qF+1thWZb17LPPWnFxcVZhYWGVx3Mc1yzc9zTLqt33iK+++spKS0uz4uLirJYtW1r33nuv5fP5IvY+XMfeDAAAAADgJDjHCQAAAADCIDgBAAAAQBgEJwAAAAAIg+AEAAAAAGEQnAAAAAAgDIITAAAAAIRBcAIAAACAMAhOAAAAABAGwQkAAAAAwiA4AQAAAEAYBCcAAAAACIPgBABoNL755hslJSXp0UcfDW5bs2aNYmJitGLFCgcrAwCYzmVZluV0EQAARMrSpUs1duxYrVmzRt27d1f//v01ZswYPfnkk06XBgAwGMEJANDoTJkyRe+9954GDx6srVu3auPGjfJ6vU6XBQAwGMEJANDoHDlyRL1799bXX3+tzZs3q0+fPk6XBAAwHOc4AQAanS+++EL79u2T3+/XV1995XQ5AIB6gBEnAECjUlZWpiFDhqh///7q3r27nnrqKW3dulWJiYlOlwYAMBjBCQDQqNx///1atGiRPv74YzVp0kTDhg1Ts2bN9NZbbzldGgDAYEzVAwA0GtnZ2Xrqqaf0yiuvKCEhQW63W6+88oo+/PBDzZ071+nyAAAGY8QJAAAAAMJgxAkAAAAAwiA4AQAAAEAYBCcAAAAACIPgBAAAAABhEJwAAAAAIAyCEwAAAACEQXACAAAAgDAITgAAAAAQBsEJAAAAAMIgOAEAAABAGAQnAAAAAAjj/wO8Cc3frATnwAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
