{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "id": "3JDc_vCt4Lkj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1ATJTpO4zb_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# FUNCIONES DE ACTIVACION Y SUS DERIVADAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g19AzOYTXYc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Clases de cada funcion (contiene su funcion y su derivada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "id": "AmUUjdrl4qSr"
   },
   "outputs": [],
   "source": [
    "class activationFunction:\n",
    "    \"\"\"\n",
    "    Class for activation functions meant to be used on Neural Networks\n",
    "    All the activation functions and its derivatives are available on the subclases\n",
    "    Subclasses:\n",
    "    Swish()\n",
    "    Relu()\n",
    "    Purelin()\n",
    "    Logsig()\n",
    "    Tansig()\n",
    "    Radbas()\n",
    "    Tribas()\n",
    "    RadBasN()\n",
    "    HardLim()\n",
    "    HardLims()\n",
    "    SatLin()\n",
    "    SatLins()\n",
    "    Softmax()\n",
    "    LeakyRelu()\n",
    "    ELU()\n",
    "    GELU()\n",
    "    PReLU()\n",
    "    SELU()\n",
    "    SiLU()\n",
    "    Softplus()\n",
    "    \"\"\"\n",
    "    def function(self,x):\n",
    "        \"\"\"Activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def derivative(self,x):\n",
    "        \"\"\"Derivative of the activation function\"\"\"\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")\n",
    "    def active(self):\n",
    "        raise NotImplementedError(\"This is only the base function, the implementation of this is on any of the other functions, for more information check the class DOCSTRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "id": "vl52Q1Rl4sbo"
   },
   "outputs": [],
   "source": [
    "class Swish(activationFunction):\n",
    "    \"\"\"Scaled Exponential Linear Unit With a Shift function\"\"\"\n",
    "    def __init__(self, beta=1):\n",
    "        self.beta = beta\n",
    "    def function(self, x):\n",
    "        return x * (1 / (1 + np.exp(-self.beta * x)))\n",
    "    def derivative(self, x):\n",
    "        return (self.beta * self.function(x)) + (1 / (1 + np.exp(-self.beta * x))) * (1 - self.beta * self.function(x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "id": "8shE1T3W4yJz"
   },
   "outputs": [],
   "source": [
    "class Relu(activationFunction):\n",
    "    \"\"\"Rectified linear unit function (ReLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.maximum(0,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,0)\n",
    "    def active(self):\n",
    "        out = [0, float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "id": "j2v-Y_2495Yb"
   },
   "outputs": [],
   "source": [
    "class Purelin(activationFunction):\n",
    "    \"\"\"Linear (Identity) function\"\"\"\n",
    "    def function(self,x):\n",
    "      return x\n",
    "    def derivative(self,x):\n",
    "      return np.ones_like(x)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "id": "KuoUYoUuIG6P"
   },
   "outputs": [],
   "source": [
    "class Logsig(activationFunction):\n",
    "    \"\"\"Logistic function\"\"\"\n",
    "    def function(self, x):\n",
    "      return 1 / (1 + np.exp(-x))\n",
    "    def derivative(self,x):\n",
    "        return self.function(x) * (1 - self.function(x))\n",
    "    def active(self):\n",
    "        out = [-4.0, 4.0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "id": "FbhNfFvmI9Cd"
   },
   "outputs": [],
   "source": [
    "class Tansig(activationFunction):\n",
    "    \"\"\"Hyperbolic function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.tanh(x)\n",
    "    def derivative(self,x):\n",
    "        return  1- np.tanh(x)**2\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "id": "jIZd2WWFKioZ"
   },
   "outputs": [],
   "source": [
    "class Radbas(activationFunction):\n",
    "    \"\"\"Gaussian function\"\"\"\n",
    "    def function(self,x):\n",
    "        return np.exp(-x**2)\n",
    "    def derivative(self,x):\n",
    "        return -2 * x * np.exp(-x**2)\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "id": "UTE_IxFnLIkU"
   },
   "outputs": [],
   "source": [
    "class Tribas(activationFunction):\n",
    "    \"\"\"Triangular basis function\"\"\"\n",
    "    def function(self, x):\n",
    "      return np.maximum(0, 1 - np.abs(x))\n",
    "    def derivative(self, x):\n",
    "      return np.where(np.abs(x) < 1, -1, 0)\n",
    "    def active(self):\n",
    "        out = [-1, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "id": "lgC1nhgnLjeX"
   },
   "outputs": [],
   "source": [
    "class RadBasN(activationFunction):\n",
    "    \"\"\"Normalized radial basis function\"\"\"\n",
    "    def __init__(self, sigma=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        sigma : float by default 1\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def function(self, x):\n",
    "      return np.exp(-0.5 * (x / self.sigma)**2)\n",
    "    def derivative(self, x):\n",
    "      return -x / self.sigma**2 * np.exp(-0.5 * (x / self.sigma)**2)\n",
    "    def active(self):\n",
    "        out = [-2, 2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "id": "yhHg-ZxGNLkp"
   },
   "outputs": [],
   "source": [
    "class HardLim(activationFunction):\n",
    "    \"\"\"Hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)\n",
    "    def active(self):\n",
    "        out = [0, 0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "id": "Vx0uAWHENjcZ"
   },
   "outputs": [],
   "source": [
    "class HardLims(activationFunction):\n",
    "    \"\"\"Symmetric hard limit function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    def derivative(self, x):\n",
    "        return np.zeros_like(x)\n",
    "    def active(self):\n",
    "        out = [0, 0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "id": "s44mofomN2ju"
   },
   "outputs": [],
   "source": [
    "class SatLin(activationFunction):\n",
    "    \"\"\"Saturatin linear function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, 0, None)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-0, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "id": "3W5XynwgN-wb"
   },
   "outputs": [],
   "source": [
    "class SatLins(activationFunction):\n",
    "    \"\"\"Symmetric saturating function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.clip(x, -1, 1)\n",
    "\n",
    "    def derivative(self, x):\n",
    "        return np.where(np.logical_and(x >= -1, x <= 1), 1, 0)\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-1, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "id": "ra-GQVY6Pm9-"
   },
   "outputs": [],
   "source": [
    "class Softmax(activationFunction):\n",
    "    \"\"\"Normalized exponential function (softmax)\"\"\"\n",
    "    def function(self, x):\n",
    "        x  = np.subtract(x, np.max(x))        # prevent overflow\n",
    "        ex = np.exp(x)\n",
    "        return ex / np.sum(ex)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        raise NotImplementedError(\"La derivada de Softmax no se utiliza tÃ­picamente en el entrenamiento de redes neuronales.\")\n",
    "    \n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "id": "pNnzzwWtP2yO"
   },
   "outputs": [],
   "source": [
    "class LeakyRelu(activationFunction):\n",
    "    \"\"\"Leaky rectified linear unit function (leakyRelu)\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,1e-2*x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,1e-2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "id": "Kj60iC_mP5aR"
   },
   "outputs": [],
   "source": [
    "class ELU(activationFunction):\n",
    "    \"\"\"Exponential Linear Unit function (ELU)\"\"\"\n",
    "    def __init__(self, alpha=1):\n",
    "        \"\"\"\n",
    "        PARAMETERS:\n",
    "        alpha = float by default 1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x>0,x,self.alpha*(np.exp(x)-1))\n",
    "    def derivative(self, x):\n",
    "        return np.where(x>0,1,self.alpha*np.exp(x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "id": "mpJ1CbgVP8hu"
   },
   "outputs": [],
   "source": [
    "class GELU(activationFunction):\n",
    "    \"\"\"Gaussian Error Linear Unit function (GELU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return 0.5 * x * (1 + erf(x / np.sqrt(2)))\n",
    "    def derivative(self, x):\n",
    "        return 0.5 * (1 + erf(x / np.sqrt(2))) + (x / np.sqrt(2 * np.pi)) * np.exp(-0.5 * x**2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "id": "I0lFxTdgP-w9"
   },
   "outputs": [],
   "source": [
    "class PReLU(activationFunction):\n",
    "    \"\"\"Parametric rectified linear unit function (PReLU)\"\"\"\n",
    "    def __init__(self, alpha=1e-1):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        alpha : float by default 1e-1\n",
    "        \"\"\"\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return np.where(x<0,self.alpha*x,x)\n",
    "    def derivative(self, x):\n",
    "        return np.where(x<0,self.alpha,1)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "id": "Zfu4-hwtQAcV"
   },
   "outputs": [],
   "source": [
    "class SELU(activationFunction):\n",
    "    \"\"\"Scaled exponential linear unit function (SELU)\"\"\"\n",
    "    def __init__(self, lamb= 1.0507, alpha=1.67326):\n",
    "        \"\"\"\n",
    "        PARAMETERS\n",
    "        lamb : float by default 1.0507\n",
    "        alpha : float by default 1.67326\n",
    "        Both are suposed to be always that value so it's recomended to not change them\n",
    "        \"\"\"\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "    def function(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*(np.exp(x)-1),x)\n",
    "    def derivative(self, x):\n",
    "        return self.lamb * np.where(x<0, self.alpha*np.exp(x),1)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "id": "iw1z8DEkQC5j"
   },
   "outputs": [],
   "source": [
    "class SiLU(activationFunction):\n",
    "    \"\"\"Sigmoid linear unit function (SiLU)\"\"\"\n",
    "    def function(self, x):\n",
    "        return (x / (1 + np.exp(-x)))\n",
    "    def derivative(self, x):\n",
    "        return (1 + np.exp(-x) + x*np.exp(-x))/((1+np.exp(-x))**2)\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "id": "CIoQz-TqQFLg"
   },
   "outputs": [],
   "source": [
    "class Softplus(activationFunction):\n",
    "    \"\"\"Smooth approximation ReLU function\"\"\"\n",
    "    def function(self, x):\n",
    "        return np.log(1 + np.exp(x))\n",
    "    def derivative(self, x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    def active(self):\n",
    "        out = [-float('inf'), float('inf')]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Funciones de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorFunctions:\n",
    "    @staticmethod\n",
    "    def MSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean squared error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def MAE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the mean absolute error between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Mean absolute error\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def SSE(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the sum of squared errors between true and predicted values.\n",
    "\n",
    "        Parameters:\n",
    "        y_true: numpy.ndarray\n",
    "            True values\n",
    "        y_pred: numpy.ndarray\n",
    "            Predicted values\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Sum of squared errors\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        return np.sum((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFBX6lkR44kf"
   },
   "source": [
    "# ESTRUCTURA DE LA RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Class for the structure of a Neural Network\"\"\"\n",
    "    def __init__(self, input_size:int, layer_sizes:list[int], output_size:int, \n",
    "                 activation_funcs:list['activationFunction'], wInit:str='random',\n",
    "                 dropout_rate:float=0)->None:\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        input_size: int \n",
    "            Defines the size of the input layer\n",
    "        layer_sizes: int array \n",
    "            Defines the sizes of the ocult layers\n",
    "        output_size: int \n",
    "            Defines the size of the output layer\n",
    "        activation_funcs: activationFunction class array \n",
    "            Defines the activation function per layer\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.layer_sizes = [input_size] + layer_sizes + [output_size]  # Incluir el tamaÃ±o de la capa de entrada y de salida\n",
    "        self.output_size = output_size\n",
    "        self.activation_funcs = activation_funcs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = len(self.layer_sizes)\n",
    "        self.weights = self._initializeWeights(wInit)\n",
    "        self.n_outputs = []  # Lista para almacenar las salidas antes de la funciÃ³n de activaciÃ³n\n",
    "        self.a_outputs = []\n",
    "        self.dropout_masks = []\n",
    "\n",
    "    \n",
    "    def _initializeWeights(self, wInit):\n",
    "        \"\"\"\n",
    "        Inicializa los pesos de la red neuronal ya sea de manera random o mediante el metodo nguyen widraw\n",
    "        \n",
    "        :return: Lista de matrices de pesos como np.ndarrar\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        if wInit == 'random':\n",
    "            for i in range(self.num_layers - 1):\n",
    "                W = np.random.randn(self.layer_sizes[i] + 1, self.layer_sizes[i+1]) # +1 para incluir los sesgos\n",
    "                weights.append(W)\n",
    "        elif wInit == 'nguyen':\n",
    "            for i in range(self.num_layers - 1):\n",
    "                ni = self.layer_sizes[i]\n",
    "                no = self.layer_sizes[i+1]\n",
    "                g = (0.7*no) ** (1/ni)\n",
    "                active = self.activation_funcs[i].active()\n",
    "                if not np.isinf(active[0]) and not np.isinf(active[1]):\n",
    "                    W = np.random.randn(no, ni)\n",
    "                    W = W / np.linalg.norm(W, axis=1, keepdims=True)  # Normalizacion\n",
    "                    W = g * W\n",
    "                    beta = np.linspace(active[0], active[1], no).reshape(-1, 1)\n",
    "                    bias = g * (np.sign(W[:, 0]).reshape(-1, 1) * beta)\n",
    "                    W = np.hstack([W, bias])\n",
    "                    weights.append(W.T)\n",
    "                else:\n",
    "                    W = g * np.random.randn(ni+1,no)\n",
    "                    weights.append(W)\n",
    "        else:\n",
    "            raise KeyError(\"No se reconoce el inicializador\")\n",
    "        return weights\n",
    "        #return np.array(weights, dtype = object)\n",
    "    \n",
    "    def forwardPass(self, inputs, training=True):\n",
    "        A = np.hstack([inputs, np.ones((inputs.shape[0], 1))])\n",
    "        self.n_outputs = [inputs]\n",
    "        self.a_outputs = [A]\n",
    "        self.dropout_masks = []\n",
    "\n",
    "        for i, weight in enumerate(self.weights):\n",
    "            Z = np.dot(A, weight)\n",
    "            self.n_outputs.append(Z)\n",
    "            A = self.activation_funcs[i].function(Z)\n",
    "            \n",
    "            if self.dropout_rate > 0 and training and i < len(self.weights) - 1:  # Dropout en todo menos la ultima capa y solo durante el entrenamiento\n",
    "                dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, size=A.shape)\n",
    "                A *= dropout_mask\n",
    "                self.dropout_masks.append(dropout_mask)\n",
    "            elif not training and i < len(self.weights) - 1:  # Escalar datos durante la inferencia (no entrenando)\n",
    "                A *= (1 - self.dropout_rate)\n",
    "            \n",
    "            A = np.hstack([A, np.ones((A.shape[0], 1))])\n",
    "            self.a_outputs.append(A)\n",
    "        return A[:, :-1]\n",
    "\n",
    "    def backwardPass(self, targets):\n",
    "        #gradients = np.array([])\n",
    "        gradients = []\n",
    "        e = targets - self.a_outputs[-1][:,:-1]\n",
    "        ge = -2*e\n",
    "        delta = ge * self.activation_funcs[-1].derivative(np.array(self.n_outputs[-1]))\n",
    "        ae = self.a_outputs[-2] #El metodo forward pass deja a_outputs aumentado\n",
    "        ge = np.dot(ae.T,delta)\n",
    "        gradients.append(ge)\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1): \n",
    "            fdx = self.activation_funcs[i].derivative(np.array(self.n_outputs[i]))\n",
    "            delta = fdx * np.dot(delta,self.weights[i][:-1].T)\n",
    "            \n",
    "            if self.dropout_rate > 0 and self.dropout_masks: #Si existe alguna mascara de dropout aplicarla\n",
    "                delta *= self.dropout_masks.pop()  # Apply dropout mask\n",
    "            \n",
    "            ae = self.a_outputs[i-1]\n",
    "            ge = np.dot(ae.T,delta)\n",
    "            gradients.insert(0,ge)\n",
    "        return gradients\n",
    "            \n",
    "    def error(self,targets,error_func):\n",
    "        \"\"\"\n",
    "        Calculate the error based on the inputs, outputs, and error function specified.\n",
    "\n",
    "        Parameters:\n",
    "        inputs: numpy.ndarray\n",
    "            Input data\n",
    "        outputs: numpy.ndarray\n",
    "            Output data\n",
    "        error_func: function\n",
    "            Error function to use (e.g., mean squared error, mean absolute error, etc.)\n",
    "\n",
    "        Returns:\n",
    "        float\n",
    "            Error value calculated using the specified error function.\n",
    "        \"\"\"\n",
    "        predicted_outputs = self.a_outputs[-1][:,:-1]\n",
    "        return error_func(targets, predicted_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Optimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"\n",
    "    Class for the optimizers based on two different algorithms\n",
    "\n",
    "    RMSProp()\n",
    "    AdamW() \n",
    "    \"\"\"\n",
    "    def __init__(self,lr:float,maxEpochs:int,goal:float,mingrad:float,nn: NeuralNetwork,\n",
    "                 inputs:np.array,targets:np.array,error_fun,show:int =1,consecutive_epochs:int =10,\n",
    "                 batch_size: int=1)->None:  \n",
    "        self.nn = nn\n",
    "        self.name = \"DEFAULT\"\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size if batch_size > 0 else inputs.shape[0]  # Si batch_size <= 0, usa todos los ejemplos\n",
    "        self.maxEpochs = maxEpochs\n",
    "        self.goal = goal\n",
    "        self.mingrad = mingrad\n",
    "        self.show = show\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.error_fun = error_fun\n",
    "        self.consecutive_epochs = consecutive_epochs\n",
    "        \n",
    "        \n",
    "    def optimize(self):\n",
    "        this = self.name\n",
    "        stop = \"\"\n",
    "        epochs = []\n",
    "        perfs  = []\n",
    "        consecutive_rise = 0  # Contador para el nÃºmero de Ã©pocas consecutivas en las que el rendimiento ha subido\n",
    "        prev_perf = float('inf')\n",
    "        num_samples = self.inputs.shape[0]  # NÃºmero de ejemplos de entrenamiento\n",
    "        print(\"\\n\")\n",
    "\n",
    "        #Entrenamiento\n",
    "        for epoch in range(self.maxEpochs+1):\n",
    "            #Mezclar los datos\n",
    "            permutation = np.random.permutation(num_samples)\n",
    "            inputs_shuffled = self.inputs[permutation, :]\n",
    "            targets_shuffled = self.targets[permutation, :]\n",
    "\n",
    "            # Procesar mini-lotes\n",
    "            for start in range(0, num_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_inputs = inputs_shuffled[start:end,: ]\n",
    "                batch_targets = targets_shuffled[start:end,:]\n",
    "\n",
    "                # Performance and Gradient\n",
    "                _ = self.nn.forwardPass(batch_inputs)\n",
    "                gX = self.nn.backwardPass(batch_targets)\n",
    "\n",
    "                # Aplanar y concatenar los gradientes en un solo vector\n",
    "                gX_flattened = np.concatenate([grad.flatten() for grad in gX])\n",
    "                  \n",
    "                self.train(gX_flattened)  # Pasar gX aplanado\n",
    "                \n",
    "            if self.batch_size != self.inputs.shape[0]:\n",
    "                _ = self.nn.forwardPass(self.inputs)\n",
    "                gX = self.nn.backwardPass(self.targets)\n",
    "                \n",
    "            perf = self.nn.error(self.targets, self.error_fun)\n",
    "            \n",
    "            # Aplanar y concatenar los gradientes en un solo vector\n",
    "            gX_flattened = np.concatenate([grad.flatten() for grad in gX])\n",
    "            normgX = np.linalg.norm(gX_flattened)\n",
    "\n",
    "            # Stopping criteria\n",
    "            epochs = np.append(epochs, epoch)\n",
    "            perfs = np.append(perfs, perf)\n",
    "            if np.all(perf <= self.goal):\n",
    "                stop = \"Performance goal met\"\n",
    "            elif epoch == self.maxEpochs:\n",
    "                stop = \"Maximum epoch reached, performance goal was not met\"\n",
    "            elif normgX < self.mingrad:\n",
    "                stop = \"Minimum gradient reached, performance goal was not met\"\n",
    "            elif perf >= prev_perf:\n",
    "                consecutive_rise += 1\n",
    "                if consecutive_rise >= self.consecutive_epochs:\n",
    "                    stop = f\"Performance has risen for {self.consecutive_epochs} consecutive epochs\"\n",
    "            elif perf < prev_perf:\n",
    "                consecutive_rise = 0\n",
    "\n",
    "            prev_perf = perf\n",
    "            if (np.fmod(epoch, self.show) == 0 or len(stop) != 0):\n",
    "                print(this, end=\": \")\n",
    "                if np.isfinite(self.maxEpochs):\n",
    "                    print(\"Epoch \", epoch, \"/\", self.maxEpochs, end=\" \")\n",
    "                if np.isfinite(self.goal):\n",
    "                    print(\", Performance %8.3e\" % perf, \"/\", self.goal, end=\" \")\n",
    "                if np.isfinite(self.mingrad):\n",
    "                    print(\", Gradient %8.3e\" % normgX, \"/\", self.mingrad)\n",
    "\n",
    "                if len(stop) != 0:\n",
    "                    print(\"\\n\", this, \":\", stop, \"\\n\")\n",
    "                    break            \n",
    "        return perfs, epochs\n",
    "\n",
    "    def train(self,gX):\n",
    "        raise NotImplementedError(\"No se ha definido el optimizador, esta es la clase base\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RmsProp(Optimizer):\n",
    "    def __init__(self, nn: NeuralNetwork, inputs:np.array, targets:np.array,lr: float =1e-3, batch_size: int =0, maxEpochs: int =500, \n",
    "                 goal: float =1e-8,mingrad: float =1e-11, show:int =1, error_fun=ErrorFunctions.SSE, \n",
    "                 consecutive_epochs: int=10,WDecay:float=0,alpha:float=0.99,centered:bool=False,\n",
    "                 momentum:float=0.6,epsilon:float=1e-9) -> None:\n",
    "        if not isinstance(inputs, np.ndarray):\n",
    "            raise TypeError(\"El argumento 'inputs' debe ser un array de NumPy.\")\n",
    "        if not isinstance(targets, np.ndarray):\n",
    "            raise TypeError(\"El argumento 'targets' debe ser un array de NumPy.\")\n",
    "        super().__init__(lr,maxEpochs,goal,mingrad,nn,inputs,targets,error_fun,show,consecutive_epochs,batch_size)\n",
    "        self.name = \"trainRMSPROP\"\n",
    "        self.epsilon = epsilon\n",
    "        self.v = np.zeros_like(np.concatenate([w.flatten() for w in nn.weights]))  # Vector de acumulaciÃ³n de gradientes\n",
    "        self.vh = 0\n",
    "        self.b = 0\n",
    "        self.gAvg = 0\n",
    "        self.WDecay = WDecay\n",
    "        self.alpha = alpha\n",
    "        self.centered = centered\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def train(self, gX):        \n",
    "        if self.WDecay != 0:\n",
    "            gX = gX + gX*self.WDecay\n",
    "        self.v = self.alpha*self.v + ((1-self.alpha)*(gX**2))\n",
    "        self.vh = self.v\n",
    "        if self.centered:\n",
    "            self.gAvg = self.gAvg*self.alpha + ((1-self.alpha)*gX)\n",
    "            self.vh = self.vh - self.gAvg**2\n",
    "        if self.momentum > 0:\n",
    "            self.b = self.momentum*self.b + gX/((self.vh**(1/2))+self.epsilon)\n",
    "            update = self.lr*self.b\n",
    "            #self.nn.weights += dX\n",
    "        else:\n",
    "            update = self.lr*(gX/((self.vh**(1/2))+1e-8))\n",
    "            #self.nn.weights += dX\n",
    "        \n",
    "        # Actualizar pesos\n",
    "        start = 0\n",
    "        for i, w in enumerate(self.nn.weights):\n",
    "            shape = w.shape\n",
    "            size = np.prod(shape)\n",
    "            grad_update = update[start:start+size].reshape(shape)\n",
    "            self.nn.weights[i] -= grad_update\n",
    "            start += size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "def main():\n",
    "    \n",
    "    activationFunction()\n",
    "    neural_network = NeuralNetwork(input_size = 2,\n",
    "                                layer_sizes = [2],\n",
    "                                output_size = 1,\n",
    "                                activation_funcs = [Swish(),Logsig()],\n",
    "                                wInit = 'nguyen')\n",
    "\n",
    "    # Carga el archivo .mat\n",
    "    inputs = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "    targets =np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "    for weight in neural_network.weights:\n",
    "        print(f\"Tamano de los pesos: {weight.shape}\")\n",
    "    \n",
    "    Optimizador = RmsProp(nn=neural_network,\n",
    "                          inputs=inputs,\n",
    "                          targets=targets,\n",
    "                          lr=1e-2,\n",
    "                          maxEpochs=500,\n",
    "                          show=50,\n",
    "                          consecutive_epochs=20,\n",
    "                          goal=1e-4,\n",
    "                          mingrad=1e-8,\n",
    "                          batch_size=1,\n",
    "                          error_fun=ErrorFunctions.MSE)\n",
    "    \n",
    "    perfs,epochs = Optimizador.optimize()\n",
    "    \"\"\"\n",
    "    \n",
    "    activationFunction()\n",
    "    neural_network = NeuralNetwork(input_size = 2,\n",
    "                                layer_sizes = [30,60],\n",
    "                                output_size = 2,\n",
    "                                activation_funcs = [LeakyRelu(),LeakyRelu(),Purelin()],\n",
    "                                wInit = 'nguyen',\n",
    "                                dropout_rate=0.15)\n",
    "\n",
    "    # Carga el archivo .mat\n",
    "    data = loadmat('engine_dataset.mat')\n",
    "    inputs = data['engineInputs'].T\n",
    "    targets = data['engineTargets'].T\n",
    "    for weight in neural_network.weights:\n",
    "        print(f\"Tamano de los pesos: {weight.shape}\")\n",
    "    \n",
    "    Optimizador = RmsProp(nn=neural_network,\n",
    "                          inputs=inputs,\n",
    "                          targets=targets,\n",
    "                          lr=1e-3,\n",
    "                          maxEpochs=2000,\n",
    "                          show=200,\n",
    "                          consecutive_epochs=5,\n",
    "                          mingrad=1e-8,\n",
    "                          batch_size=250,\n",
    "                          error_fun=ErrorFunctions.MSE)\n",
    "    \n",
    "    perfs,epochs = Optimizador.optimize()\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, perfs)\n",
    "    plt.title('Performance')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "         \n",
    "    outputs = neural_network.forwardPass(inputs,False)\n",
    "    print(f\"Salida:{outputs}\")\n",
    "    print(\"PredicciÃ³n binaria:\")\n",
    "    for fila in outputs.T:  # Iterar sobre las filas de la matriz de salida\n",
    "        for valor in fila:\n",
    "            if valor > 0.5:\n",
    "                print(\"1\", end=\" \")\n",
    "            else:\n",
    "                print(\"0\", end=\" \")\n",
    "    resultado_esperado = [0, 1, 1, 0]\n",
    "    print(\"\\nResultado esperado:\\n \", resultado_esperado)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamano de los pesos: (3, 2)\n",
      "Tamano de los pesos: (3, 1)\n",
      "\n",
      "\n",
      "trainRMSPROP: Epoch  0 / 500 , Performance 4.440e-01 / 0.0001 , Gradient 2.398e-01 / 1e-08\n",
      "trainRMSPROP: Epoch  50 / 500 , Performance 3.779e-02 / 0.0001 , Gradient 8.988e-02 / 1e-08\n",
      "trainRMSPROP: Epoch  100 / 500 , Performance 1.050e-02 / 0.0001 , Gradient 2.695e-02 / 1e-08\n",
      "trainRMSPROP: Epoch  150 / 500 , Performance 5.166e-03 / 0.0001 , Gradient 1.603e-02 / 1e-08\n",
      "trainRMSPROP: Epoch  200 / 500 , Performance 1.987e-03 / 0.0001 , Gradient 6.706e-03 / 1e-08\n",
      "trainRMSPROP: Epoch  250 / 500 , Performance 6.948e-04 / 0.0001 , Gradient 2.761e-03 / 1e-08\n",
      "trainRMSPROP: Epoch  300 / 500 , Performance 2.456e-04 / 0.0001 , Gradient 1.026e-03 / 1e-08\n",
      "trainRMSPROP: Epoch  345 / 500 , Performance 9.801e-05 / 0.0001 , Gradient 4.360e-04 / 1e-08\n",
      "\n",
      " trainRMSPROP : Performance goal met \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida:[[0.00906776]\n",
      " [0.99619697]\n",
      " [0.98325086]\n",
      " [0.00384991]]\n",
      "PredicciÃ³n binaria:\n",
      "0 1 1 0 \n",
      "Resultado esperado:\n",
      "  [0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
